#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
# ]
# ///

import argparse
import json
import subprocess
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path

CONFIG_PATH = Path.home() / ".config" / "fast-commit" / ".env"
ERRORS_DIR = Path.home() / ".fastc" / "errors"


def record_error(message, context=None):
    """Record error details to disk for debugging.

    Creates timestamped error files in the errors/ directory with:
    - Error message
    - Timestamp
    - Git state (branch, status)
    - Optional context dict
    - Stack trace
    """
    ERRORS_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    error_file = ERRORS_DIR / f"error_{timestamp}.txt"

    lines = [
        f"timestamp: {datetime.now().isoformat()}",
        f"error: {message}",
        "",
    ]

    # Capture git state
    try:
        branch_result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True, timeout=5
        )
        branch = branch_result.stdout.decode().strip() if branch_result.returncode == 0 else "unknown"
        lines.append(f"git_branch: {branch}")

        status_result = subprocess.run(
            ["git", "status", "--short"],
            capture_output=True, timeout=5
        )
        if status_result.returncode == 0:
            status = status_result.stdout.decode().strip()
            lines.append(f"git_status:\n{status}")
    except Exception:
        lines.append("git_state: failed to capture")

    lines.append("")

    # Add context if provided
    if context:
        lines.append("context:")
        for key, value in context.items():
            lines.append(f"  {key}: {value}")
        lines.append("")

    # Add stack trace
    lines.append("stack_trace:")
    lines.append(traceback.format_exc())

    error_file.write_text("\n".join(lines))
    print(f"  error recorded: {error_file}", file=sys.stderr)


def exit_with_error(message, context=None):
    """Print error, record to disk, and exit."""
    print(f"error: {message}", file=sys.stderr)
    record_error(message, context)
    sys.exit(1)

# Files to exclude from diff analysis (don't help with commit messages)
EXCLUDED_PATTERNS = [
    # JS/Node lockfiles
    "pnpm-lock.yaml",
    "package-lock.json",
    "yarn.lock",
    "bun.lockb",
    # Other language lockfiles
    "Cargo.lock",
    "poetry.lock",
    "Pipfile.lock",
    "go.sum",
    "composer.lock",
    "Gemfile.lock",
    # Minified/generated files
    "*.min.js",
    "*.min.css",
    "*.map",
]

SYSTEM_PROMPT = """You are a git commit message generator. Given a git diff, analyse the changes and group them into logical atomic commits.

CRITICAL REQUIREMENTS:
- "files" MUST be actual file paths from the diff (e.g., "src/auth.go", "tests/auth_test.go")
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"
- Every file in the diff MUST appear in exactly one commit's files array

Rules:
- Each commit should represent one logical change
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single commit

BAD OUTPUT (will be rejected):
[
  {"files": "files", "message": "message", "description": "description"}
]

GOOD OUTPUT:
[
  {
    "files": ["src/auth.go", "tests/auth_test.go"],
    "message": "feat(auth): add login endpoint",
    "description": "Users need to authenticate before accessing protected resources."
  }
]

Respond with ONLY a valid JSON array (no markdown, no explanation):"""

PHASE1_PROMPT = """You are a git commit analyzer. Given a summary of changed files (name-status and stats), group them into logical atomic commits.

CRITICAL REQUIREMENT:
- "files" MUST be actual file paths from the FILE CHANGES list below

Rules:
- Each group should represent one logical change
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single group
- Use the file paths and change types (A=added, M=modified, D=deleted, R=renamed) to understand the changes

BAD OUTPUT (will be rejected):
[{"files": "files", "hint": "hint"}]

GOOD OUTPUT:
[{"files": ["src/auth.go", "tests/auth_test.go"], "hint": "authentication logic changes"}]

Respond with ONLY a valid JSON array of file groups (no markdown, no explanation):"""

PHASE2_PROMPT = """You are a git commit message generator. Given a diff for a specific group of related files, generate ONE commit message.

CRITICAL REQUIREMENTS:
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"

Rules:
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed

BAD OUTPUT (will be rejected):
{"message": "message", "description": "description"}

GOOD OUTPUT:
{"message": "feat(auth): add login endpoint", "description": "Users need to authenticate before accessing protected resources."}

Respond with ONLY a valid JSON object (no markdown, no explanation):"""


def load_config():
    if not CONFIG_PATH.exists():
        exit_with_error(
            f"config not found at {CONFIG_PATH}. Create it with OPENROUTER_API_KEY and MODEL",
            {"config_path": str(CONFIG_PATH)}
        )
    config = {}
    for line in CONFIG_PATH.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        key, _, value = line.partition("=")
        config[key.strip()] = value.strip().strip("\"'")
    if "OPENROUTER_API_KEY" not in config:
        exit_with_error("OPENROUTER_API_KEY not set in config", {"config_path": str(CONFIG_PATH)})
    if "MODEL" not in config:
        exit_with_error("MODEL not set in config", {"config_path": str(CONFIG_PATH)})
    return config


def run(cmd, check=False):
    result = subprocess.run(cmd, capture_output=True)
    stdout = result.stdout.decode("utf-8", errors="replace")
    stderr = result.stderr.decode("utf-8", errors="replace")
    if check and result.returncode != 0:
        exit_with_error(
            f"command failed: {' '.join(cmd)}",
            {"command": " ".join(cmd), "stderr": stderr.strip(), "returncode": result.returncode}
        )
    return stdout, stderr, result.returncode


def build_pathspec_excludes():
    """Build git pathspec exclude args for lockfiles etc."""
    excludes = []
    for pattern in EXCLUDED_PATTERNS:
        excludes.extend([":(exclude)" + pattern])
    return excludes


def strip_diff_noise(diff):
    """Remove noise lines from diff to save tokens."""
    lines = []
    for line in diff.splitlines():
        if line.startswith("index "):
            continue
        if line.startswith("similarity index "):
            continue
        if line.startswith("dissimilarity index "):
            continue
        lines.append(line)
    return "\n".join(lines)


def get_diff():
    """Get diff for analysis, excluding lockfiles from LLM context."""
    excludes = build_pathspec_excludes()
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    if staged.strip():
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "staged"
    diff, _, _ = run(["git", "diff", "--minimal"])
    if not diff.strip():
        untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
        if not untracked.strip():
            return "", "none"
        run(["git", "add", "-A"], check=True)
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "all"
    run(["git", "add", "-A"], check=True)
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
    return strip_diff_noise(diff), "all"


MAX_DIFF_CHARS = 20000
MAX_LINES_PER_HUNK = 15
MAX_HUNKS_PER_FILE = 5
LARGE_DIFF_FILES = 15
MAX_OUTPUT_TOKENS = 4096


def parse_files_from_diff(diff):
    """Extract the set of files mentioned in a diff."""
    files = set()
    for line in diff.splitlines():
        if line.startswith("diff --git a/"):
            parts = line.split(" b/")
            if len(parts) == 2:
                files.add(parts[1])
        elif line.startswith("rename from "):
            files.add(line[12:])
        elif line.startswith("rename to "):
            files.add(line[10:])
    return files


def is_lockfile(path):
    """Check if a file path matches any lockfile pattern."""
    for pattern in EXCLUDED_PATTERNS:
        if pattern.startswith("*"):
            if path.endswith(pattern[1:]):
                return True
        elif path.endswith("/" + pattern) or path == pattern:
            return True
    return False


def get_staged_files_for_commit():
    """Get actual staged files that can be committed (excluding lockfiles).

    Returns dict mapping path -> {old_path, is_deleted}
    This ensures we use paths that actually exist for git add/rm.
    Lockfiles are excluded since they're committed separately.
    """
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M"])
    files = {}
    for line in name_status.strip().splitlines():
        if not line:
            continue
        parts = line.split("\t")
        status = parts[0]
        if status.startswith("R"):  # Rename: R100\told\tnew
            old_path = parts[1]
            new_path = parts[2]
            if not is_lockfile(new_path):
                files[new_path] = {"old": old_path, "deleted": False}
        elif status == "D":  # Deleted
            if not is_lockfile(parts[1]):
                files[parts[1]] = {"old": parts[1], "deleted": True}
        else:  # Added, Modified, etc
            if not is_lockfile(parts[1]):
                files[parts[1]] = {"old": parts[1], "deleted": False}
    return files


def normalize_llm_files(llm_files, staged_files):
    """Map LLM file paths to actual staged paths.

    The LLM might return old paths for renamed files or paths that don't exist.
    This maps them to actual paths that git can work with.
    """
    # Build reverse lookup: old_path -> new_path
    old_to_new = {info["old"]: path for path, info in staged_files.items()}

    normalized = []
    for f in llm_files:
        if f in staged_files:
            # File exists as-is (new path or unchanged)
            normalized.append(f)
        elif f in old_to_new:
            # LLM returned old path of renamed file, use new path
            normalized.append(old_to_new[f])
        # Skip files that don't exist at all (hallucinated or already committed)
    return normalized


def is_file_deleted(path, staged_files):
    """Check if a file is marked as deleted in staged files."""
    info = staged_files.get(path)
    return info["deleted"] if info else False


def is_echoing_keys(commit):
    """Check if commit has literal key strings instead of real values."""
    if not isinstance(commit, dict):
        return True
    files = commit.get("files")
    message = commit.get("message")
    if isinstance(files, str) and files in ("files", "[]", ""):
        return True
    if isinstance(message, str) and message in ("message", ""):
        return True
    return False


def validate_and_fix_commits(commits, staged_files):
    """Validate LLM output and fix issues.

    - Normalizes file paths (handles renames)
    - Removes invalid files
    - Errors if any files are missing from groupings
    - Returns fixed list of commits
    """
    # Detect key-echoing pattern (model failure)
    echo_count = sum(1 for c in commits if is_echoing_keys(c))
    if echo_count > 0 and echo_count == len(commits):
        exit_with_error(
            f"LLM returned malformed commits (echoing keys: {echo_count}/{len(commits)})",
            {"commits_sample": commits[:3]}
        )

    all_staged = set(staged_files.keys())
    covered_files = set()
    fixed_commits = []

    for commit in commits:
        if "files" not in commit or "message" not in commit:
            print(f"  warning: skipping malformed commit: {commit}", file=sys.stderr)
            continue

        normalized = normalize_llm_files(commit["files"], staged_files)
        if not normalized:
            print(f"  warning: skipping commit with no valid files: {commit.get('message', '<missing>')}", file=sys.stderr)
            continue

        covered_files.update(normalized)
        fixed_commits.append({
            "files": normalized,
            "message": commit["message"],
            "description": commit.get("description", ""),
        })

    missing_files = all_staged - covered_files
    if missing_files:
        exit_with_error(
            f"LLM failed to group {len(missing_files)} files: {list(missing_files)[:5]}{'...' if len(missing_files) > 5 else ''}",
            {"missing_files": list(missing_files), "staged_files": list(all_staged)}
        )

    return fixed_commits


def get_diff_for_files(files):
    """Get the diff for specific files only."""
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + list(files))
    return strip_diff_noise(diff)


def compress_diff(diff):
    """Compress diff while preserving semantic information.

    Strategy:
    - Include --stat and --dirstat for overview
    - Keep all hunk headers (@@...@@) which show function context
    - Limit lines per hunk, prioritizing additions (+) over context
    - Limit total hunks per file
    """
    excludes = build_pathspec_excludes()
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)
    dirstat, _, _ = run(["git", "diff", "--cached", "--dirstat", "--"] + excludes)

    parts = [f"DIFF STAT:\n{stat}"]
    if dirstat.strip():
        parts.append(f"DIRECTORY CHANGES:\n{dirstat}")
    parts.append("COMPRESSED PATCHES:")

    current_file_header = None
    current_file_lines = []
    hunk_count = 0
    hunk_lines = 0
    hunk_truncated = False

    def flush_file():
        nonlocal current_file_header, current_file_lines, hunk_count
        if current_file_header:
            parts.append(current_file_header)
            parts.extend(current_file_lines)
            if hunk_count > MAX_HUNKS_PER_FILE:
                parts.append(f"[... {hunk_count - MAX_HUNKS_PER_FILE} more hunks truncated]")
        current_file_header = None
        current_file_lines = []
        hunk_count = 0

    for line in diff.splitlines():
        if line.startswith("diff --git"):
            flush_file()
            current_file_header = line
            hunk_count = 0
        elif line.startswith("@@") and " @@" in line:
            hunk_count += 1
            hunk_lines = 0
            hunk_truncated = False
            if hunk_count <= MAX_HUNKS_PER_FILE:
                current_file_lines.append(line)
        elif current_file_header and hunk_count <= MAX_HUNKS_PER_FILE:
            if line.startswith("---") or line.startswith("+++"):
                current_file_lines.append(line)
            elif line.startswith("rename ") or line.startswith("new file") or line.startswith("deleted file"):
                current_file_lines.append(line)
            elif hunk_lines < MAX_LINES_PER_HUNK:
                if line.startswith("+") or line.startswith("-"):
                    current_file_lines.append(line)
                    hunk_lines += 1
                elif line.startswith(" ") and hunk_lines < MAX_LINES_PER_HUNK // 2:
                    current_file_lines.append(line)
                    hunk_lines += 1
            elif not hunk_truncated:
                current_file_lines.append("[... hunk truncated]")
                hunk_truncated = True

    flush_file()
    return "\n".join(parts)


RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}
MAX_RETRIES = 3


def try_repair_json(content):
    """Attempt to repair truncated JSON arrays/objects.

    Common truncation patterns:
    - Array cut mid-string: [..., "file  -> close string and array
    - Array cut mid-object: [..., {"files": ["a"]  -> close object and array
    """
    content = content.rstrip()

    # Try as-is first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try adding closing brackets/braces
    repairs = [
        '"}]',      # truncated mid-string in array
        '"]}}]',    # truncated mid-array in object in array
        '"}]}]',    # truncated mid-string in nested structure
        '"]',       # truncated mid-array
        '"}',       # truncated mid-string in object
        '}]',       # truncated mid-object in array
        ']',        # truncated array
        '}',        # truncated object
    ]

    for repair in repairs:
        try:
            result = json.loads(content + repair)
            print(f"  repaired truncated JSON with: {repair}")
            return result
        except json.JSONDecodeError:
            continue

    # Try removing the last incomplete element
    # Find the last complete element (ends with }, or ])
    for end_char in ['},', '],', '}', ']']:
        idx = content.rfind(end_char)
        if idx > 0:
            truncated = content[:idx + len(end_char)]
            # Add closing brackets as needed
            for close in [']', ']}', '}]', ']}]']:
                try:
                    result = json.loads(truncated + close)
                    print(f"  repaired truncated JSON by removing incomplete element")
                    return result
                except json.JSONDecodeError:
                    continue

    return None


def call_llm_raw(config, system_prompt, user_content):
    """Make a raw LLM API call and return parsed JSON."""
    import requests

    use_structured = config.get("STRUCTURED_OUTPUT", "true").lower() == "true"

    request_body = {
        "model": config["MODEL"],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
        "temperature": 0,
        "max_tokens": MAX_OUTPUT_TOKENS,
    }
    if use_structured:
        request_body["response_format"] = {"type": "json_object"}

    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            resp = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config['OPENROUTER_API_KEY']}",
                    "Content-Type": "application/json",
                },
                json=request_body,
                timeout=60,
            )
            if resp.status_code == 200:
                break
            if resp.status_code in RETRYABLE_STATUS_CODES:
                last_error = f"API returned {resp.status_code}: {resp.text[:200]}"
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2**attempt
                    print(f"  retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})...", file=sys.stderr)
                    time.sleep(wait_time)
                    continue
            exit_with_error(
                f"LLM API returned {resp.status_code}",
                {"status_code": resp.status_code, "response": resp.text[:500], "model": config["MODEL"]}
            )
        except requests.exceptions.Timeout:
            last_error = "request timed out"
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                print(f"  request timed out, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})...", file=sys.stderr)
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request timed out after {MAX_RETRIES} attempts",
                {"model": config["MODEL"], "attempts": MAX_RETRIES}
            )
        except requests.exceptions.RequestException as e:
            last_error = str(e)
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                print(f"  request failed: {e}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})...", file=sys.stderr)
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request failed after {MAX_RETRIES} attempts: {e}",
                {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": str(e)}
            )
    else:
        exit_with_error(
            f"LLM API failed after {MAX_RETRIES} attempts: {last_error}",
            {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": last_error}
        )
    try:
        resp_json = resp.json()
    except json.JSONDecodeError as e:
        exit_with_error(
            f"failed to parse API response as JSON: {e}",
            {"response_text": resp.text[:1000], "model": config["MODEL"]}
        )
    try:
        content = resp_json["choices"][0]["message"]["content"]
    except (KeyError, IndexError) as e:
        exit_with_error(
            f"unexpected API response structure: {e}",
            {"response_json": str(resp_json)[:1000], "model": config["MODEL"]}
        )
    content = content.strip()
    if content.startswith("```"):
        content = "\n".join(content.split("\n")[1:])
    if content.endswith("```"):
        content = "\n".join(content.split("\n")[:-1])
    content = content.strip()

    # Check if response was truncated (finish_reason)
    finish_reason = resp_json.get("choices", [{}])[0].get("finish_reason", "")
    if finish_reason == "length":
        print("  warning: LLM response was truncated due to length limit")

    # Try normal parse first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try to repair truncated JSON
    repaired = try_repair_json(content)
    if repaired is not None:
        return repaired

    exit_with_error(
        "LLM returned unparseable JSON",
        {"content": content[:1000], "model": config["MODEL"]}
    )


RETRY_PROMPT = """You MUST output REAL file paths and REAL commit messages.

Example of what NOT to do (will be rejected):
[{"files": "files", "message": "message", "description": "description"}]

Example of what you MUST do:
[{"files": ["src/main.go", "src/config.go"], "message": "feat(config): add app settings", "description": "Add runtime configuration support"}]

CRITICAL: Every file in the diff below must appear in the files array.
Output ONLY valid JSON array with no markdown fences:"""


def is_malformed_response(result):
    """Check if response is clearly malformed (key echoing or invalid structure)."""
    if not isinstance(result, list):
        return True
    if not result:
        return True
    # Check if all commits have key-echoing pattern
    echo_count = sum(1 for r in result if is_echoing_keys(r))
    return echo_count > len(result) // 2


def call_llm(config, diff):
    """Single-phase: send diff and get commits."""
    content = diff if len(diff) <= MAX_DIFF_CHARS else compress_diff(diff)
    result = call_llm_raw(config, SYSTEM_PROMPT, content)

    # Retry with stronger prompt if response looks malformed
    if is_malformed_response(result):
        print("  response looks malformed, retrying with stronger prompt...")
        result = call_llm_raw(config, RETRY_PROMPT, content[:5000])

    return result


def call_llm_two_phase(config, staged_files):
    """Two-phase approach for large diffs with many files.

    Phase 1: Send file summary (name-status + stat) to get logical groupings
    Phase 2: For each group, send actual diff to generate commit message
    """
    excludes = build_pathspec_excludes()
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M", "--"] + excludes)
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)

    phase1_content = f"FILE CHANGES:\n{name_status}\nSTATISTICS:\n{stat}"
    print("  phase 1: grouping files...")

    groups = call_llm_raw(config, PHASE1_PROMPT, phase1_content)

    # Handle case where LLM returns a dict with "groups" key instead of array
    if isinstance(groups, dict):
        groups = groups.get("groups", groups.get("commits", []))

    if not isinstance(groups, list):
        exit_with_error(
            "LLM phase1 returned non-list response",
            {"response_type": type(groups).__name__, "staged_files": list(staged_files.keys())}
        )

    commits = []
    for i, group in enumerate(groups):
        if not isinstance(group, dict):
            continue
        files = group.get("files", [])
        if not files:
            continue

        # Normalize files to actual staged paths
        normalized = normalize_llm_files(files, staged_files)
        if not normalized:
            continue

        print(f"  phase 2: generating message for group {i + 1}/{len(groups)}...")
        group_diff = get_diff_for_files(normalized)
        if not group_diff.strip():
            continue
        content = group_diff if len(group_diff) <= MAX_DIFF_CHARS else compress_diff(group_diff)
        result = call_llm_raw(config, PHASE2_PROMPT, content)

        # Handle various response formats
        if not isinstance(result, dict):
            exit_with_error(
                f"LLM phase2 returned non-dict response for group {i + 1}",
                {"response_type": type(result).__name__, "files": normalized}
            )
        message = result.get("message")
        if not message:
            exit_with_error(
                f"LLM phase2 missing message field for group {i + 1}",
                {"files": normalized, "response": result}
            )
        description = result.get("description", "")

        commits.append({"files": normalized, "message": message, "description": description})

    return commits


def get_lockfiles_in_staging():
    """Get list of lockfiles that are staged."""
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    return [f for f in staged.strip().splitlines() if f and is_lockfile(f)]


def main():
    parser = argparse.ArgumentParser(description="Fast commit with LLM-generated messages")
    parser.add_argument("--no-verify", action="store_true", help="bypass pre-commit hooks")
    parser.add_argument("--dry-run", action="store_true", help="preview commits without executing")
    parser.add_argument("--no-push", action="store_true", help="skip automatic push")
    args = parser.parse_args()

    config = load_config()

    diff, source = get_diff()
    if not diff:
        print("nothing to commit")
        return

    staged_files = get_staged_files_for_commit()
    file_count = len(staged_files)

    if file_count == 0:
        print("nothing to commit")
        return

    # Check for lockfiles before we reset staging (must happen before reset)
    lockfiles = get_lockfiles_in_staging() if source == "all" else []

    print(f"analysing {source} changes ({file_count} files)...")
    if file_count > LARGE_DIFF_FILES:
        print("  using two-phase approach for large diff")
        commits = call_llm_two_phase(config, staged_files)
    else:
        commits = call_llm(config, diff)
    commits = validate_and_fix_commits(commits, staged_files)

    if not commits:
        exit_with_error("no valid commits generated", {"file_count": file_count})

    if args.dry_run:
        print("\ndry run - would create these commits:")
        for commit in commits:
            print(f"  {commit['message']}")
            if commit.get("description"):
                print(f"    {commit['description']}")
            for f in commit["files"]:
                print(f"    - {f}")
        if lockfiles:
            print(f"  chore: update lockfiles")
            for f in lockfiles:
                print(f"    - {f}")
        return

    if source == "all":
        run(["git", "reset", "HEAD"], check=True)

    commit_args = ["git", "commit"]
    if args.no_verify:
        commit_args.append("--no-verify")

    for commit in commits:
        files = commit["files"]
        message = commit["message"]
        description = commit.get("description", "")

        # Stage files - use git rm for deleted files, git add for others
        added_files = []
        for f in files:
            if is_file_deleted(f, staged_files):
                _, stderr, code = run(["git", "rm", "--", f])
            else:
                _, stderr, code = run(["git", "add", "--", f])
            if code == 0:
                added_files.append(f)
            else:
                print(f"  warning: could not stage {f}: {stderr.strip()}", file=sys.stderr)

        if not added_files:
            print(f"  skipping commit (no files staged): {message}")
            continue

        # Use two -m flags: first for subject, second for body
        commit_cmd = commit_args + ["-m", message]
        if description:
            commit_cmd += ["-m", description]
        _, stderr, code = run(commit_cmd)
        if code != 0:
            exit_with_error(
                f"commit failed: {stderr}",
                {"message": message, "files": added_files, "stderr": stderr}
            )
        print(f"  {message}")

    # Commit lockfiles separately with a simple message
    if lockfiles:
        run(["git", "add", "--"] + lockfiles, check=True)
        # Verify something is actually staged before committing
        staged_check, _, _ = run(["git", "diff", "--cached", "--name-only"])
        if staged_check.strip():
            _, stderr, code = run(commit_args + ["-m", "chore: update lockfiles"])
            if code != 0:
                exit_with_error(
                    f"lockfile commit failed: {stderr}",
                    {"lockfiles": lockfiles, "stderr": stderr}
                )
            print("  chore: update lockfiles")

    if args.no_push:
        print("done (push skipped)")
        return

    _, stderr, code = run(["git", "push"])
    if code != 0:
        exit_with_error("push failed", {"stderr": stderr})
    print("pushed")


if __name__ == "__main__":
    main()

