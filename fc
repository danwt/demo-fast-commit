#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
# ]
# ///

import argparse
import json
import subprocess
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path

CONFIG_PATH = Path.home() / ".config" / "fast-commit" / ".env"
LOGS_DIR = Path.home() / ".fastc" / "logs"

# Session log file - created once per run
_session_log = None


def get_session_log():
    """Get or create session log file for this execution."""
    global _session_log
    if _session_log is None:
        LOGS_DIR.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        _session_log = LOGS_DIR / f"session_{timestamp}.log"
        # Write header
        _session_log.write_text(f"=== fastc session started: {datetime.now().isoformat()} ===\n")
    return _session_log


def log(level, message, context=None):
    """Log a message to the session log file.

    Levels: DEBUG, INFO, ERROR
    """
    log_file = get_session_log()
    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] {level}: {message}"
    if context:
        context_str = ", ".join(f"{k}={v}" for k, v in context.items())
        line += f" | {context_str}"
    with open(log_file, "a") as f:
        f.write(line + "\n")


MAX_STATUS_LINES_IN_LOG = 100


def log_git_state():
    """Log current git state to session log."""
    try:
        branch_result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True, timeout=5
        )
        branch = branch_result.stdout.decode().strip() if branch_result.returncode == 0 else "unknown"
        log("INFO", f"git branch: {branch}")

        status_result = subprocess.run(
            ["git", "status", "--short"],
            capture_output=True, timeout=5
        )
        if status_result.returncode == 0:
            status = status_result.stdout.decode().strip()
            if status:
                lines = status.splitlines()
                if len(lines) > MAX_STATUS_LINES_IN_LOG:
                    truncated = "\n".join(lines[:MAX_STATUS_LINES_IN_LOG])
                    log("INFO", f"git status (truncated, {len(lines)} files total):\n{truncated}\n... and {len(lines) - MAX_STATUS_LINES_IN_LOG} more")
                else:
                    log("INFO", f"git status:\n{status}")
    except Exception as e:
        log("INFO", f"failed to capture git state: {e}")


def warn(message, context=None):
    """Print warning to user and log it."""
    print(f"  warning: {message}", file=sys.stderr)
    log("INFO", message, context)


def record_error(message, context=None):
    """Record error details to session log with full context."""
    log("ERROR", message, context)

    # Also add stack trace if we're in an exception handler
    exc_info = traceback.format_exc()
    if exc_info and exc_info.strip() != "NoneType: None":
        log_file = get_session_log()
        with open(log_file, "a") as f:
            f.write(f"Stack trace:\n{exc_info}\n")

    # Log git state for context
    log_git_state()

    print(f"  session log: {get_session_log()}", file=sys.stderr)


def exit_with_error(message, context=None):
    """Print error, record to disk, and exit."""
    print(f"error: {message}", file=sys.stderr)
    record_error(message, context)
    sys.exit(1)

# Files to exclude from diff analysis (don't help with commit messages)
EXCLUDED_PATTERNS = [
    # JS/Node lockfiles
    "pnpm-lock.yaml",
    "package-lock.json",
    "yarn.lock",
    "bun.lockb",
    # Other language lockfiles
    "Cargo.lock",
    "poetry.lock",
    "Pipfile.lock",
    "go.sum",
    "composer.lock",
    "Gemfile.lock",
    # Minified/generated files
    "*.min.js",
    "*.min.css",
    "*.map",
]

SYSTEM_PROMPT = """You are a git commit message generator. Given a git diff, analyse the changes and group them into logical atomic commits.

CRITICAL REQUIREMENTS:
- "files" MUST be actual file paths from the diff (e.g., "src/auth.go", "tests/auth_test.go")
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"
- Every file in the diff MUST appear in exactly one commit's files array

Rules:
- Each commit should represent one logical change
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single commit

BAD OUTPUT (will be rejected):
[
  {"files": "files", "message": "message", "description": "description"}
]

GOOD OUTPUT:
[
  {
    "files": ["src/auth.go", "tests/auth_test.go"],
    "message": "feat(auth): add login endpoint",
    "description": "Users need to authenticate before accessing protected resources."
  }
]

Respond with ONLY a valid JSON array (no markdown, no explanation):"""

PHASE1_PROMPT = """You are a git commit analyzer. Given a summary of changed files (name-status and stats), group them into logical atomic commits.

CRITICAL REQUIREMENT:
- "files" MUST be actual file paths from the FILE CHANGES list below

Rules:
- Each group should represent one logical change
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single group
- Use the file paths and change types (A=added, M=modified, D=deleted, R=renamed) to understand the changes

BAD OUTPUT (will be rejected):
[{"files": "files", "hint": "hint"}]

GOOD OUTPUT:
[{"files": ["src/auth.go", "tests/auth_test.go"], "hint": "authentication logic changes"}]

Respond with ONLY a valid JSON array of file groups (no markdown, no explanation):"""

PHASE2_PROMPT = """You are a git commit message generator. Given a diff for a specific group of related files, generate ONE commit message.

CRITICAL REQUIREMENTS:
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"

Rules:
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed

BAD OUTPUT (will be rejected):
{"message": "message", "description": "description"}

GOOD OUTPUT:
{"message": "feat(auth): add login endpoint", "description": "Users need to authenticate before accessing protected resources."}

Respond with ONLY a valid JSON object (no markdown, no explanation):"""


def load_config():
    if not CONFIG_PATH.exists():
        exit_with_error(
            f"config not found at {CONFIG_PATH}. Create it with OPENROUTER_API_KEY and MODEL",
            {"config_path": str(CONFIG_PATH)}
        )
    config = {}
    for line in CONFIG_PATH.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        key, _, value = line.partition("=")
        config[key.strip()] = value.strip().strip("\"'")
    if "OPENROUTER_API_KEY" not in config:
        exit_with_error("OPENROUTER_API_KEY not set in config", {"config_path": str(CONFIG_PATH)})
    if "MODEL" not in config:
        exit_with_error("MODEL not set in config", {"config_path": str(CONFIG_PATH)})
    return config


def run(cmd, check=False):
    result = subprocess.run(cmd, capture_output=True)
    stdout = result.stdout.decode("utf-8", errors="replace")
    stderr = result.stderr.decode("utf-8", errors="replace")
    if check and result.returncode != 0:
        exit_with_error(
            f"command failed: {' '.join(cmd)}",
            {"command": " ".join(cmd), "stderr": stderr.strip(), "returncode": result.returncode}
        )
    return stdout, stderr, result.returncode


def build_pathspec_excludes():
    """Build git pathspec exclude args for lockfiles etc."""
    excludes = []
    for pattern in EXCLUDED_PATTERNS:
        excludes.extend([":(exclude)" + pattern])
    return excludes


def strip_diff_noise(diff):
    """Remove noise lines from diff to save tokens."""
    lines = []
    for line in diff.splitlines():
        if line.startswith("index "):
            continue
        if line.startswith("similarity index "):
            continue
        if line.startswith("dissimilarity index "):
            continue
        lines.append(line)
    return "\n".join(lines)


def get_diff():
    """Get diff for analysis, excluding lockfiles from LLM context."""
    excludes = build_pathspec_excludes()
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    if staged.strip():
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "staged"
    diff, _, _ = run(["git", "diff", "--minimal"])
    if not diff.strip():
        untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
        if not untracked.strip():
            return "", "none"
        run(["git", "add", "-A"], check=True)
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "all"
    run(["git", "add", "-A"], check=True)
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
    return strip_diff_noise(diff), "all"


MAX_DIFF_CHARS = 20000
MAX_LINES_PER_HUNK = 15
MAX_HUNKS_PER_FILE = 5
LARGE_DIFF_FILES = 15
MAX_OUTPUT_TOKENS = 4096
# Beyond this many files, skip LLM entirely and use bulk commit
BULK_FILE_THRESHOLD = 200
# Maximum characters for phase 1 file summary before fallback
MAX_PHASE1_CHARS = 50000


def parse_files_from_diff(diff):
    """Extract the set of files mentioned in a diff."""
    files = set()
    for line in diff.splitlines():
        if line.startswith("diff --git a/"):
            parts = line.split(" b/")
            if len(parts) == 2:
                files.add(parts[1])
        elif line.startswith("rename from "):
            files.add(line[12:])
        elif line.startswith("rename to "):
            files.add(line[10:])
    return files


def is_lockfile(path):
    """Check if a file path matches any lockfile pattern."""
    for pattern in EXCLUDED_PATTERNS:
        if pattern.startswith("*"):
            if path.endswith(pattern[1:]):
                return True
        elif path.endswith("/" + pattern) or path == pattern:
            return True
    return False


def get_staged_files_for_commit():
    """Get actual staged files that can be committed (excluding lockfiles).

    Returns dict mapping path -> {old_path, is_deleted}
    This ensures we use paths that actually exist for git add/rm.
    Lockfiles are excluded since they're committed separately.
    """
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M"])
    files = {}
    for line in name_status.strip().splitlines():
        if not line:
            continue
        parts = line.split("\t")
        status = parts[0]
        if status.startswith("R"):  # Rename: R100\told\tnew
            old_path = parts[1]
            new_path = parts[2]
            if not is_lockfile(new_path):
                files[new_path] = {"old": old_path, "deleted": False}
        elif status == "D":  # Deleted
            if not is_lockfile(parts[1]):
                files[parts[1]] = {"old": parts[1], "deleted": True}
        else:  # Added, Modified, etc
            if not is_lockfile(parts[1]):
                files[parts[1]] = {"old": parts[1], "deleted": False}
    return files


def normalize_llm_files(llm_files, staged_files):
    """Map LLM file paths to actual staged paths.

    The LLM might return old paths for renamed files or paths that don't exist.
    This maps them to actual paths that git can work with.
    Returns deduplicated list.
    """
    if not isinstance(llm_files, list):
        return []

    # Build reverse lookup: old_path -> new_path
    old_to_new = {info["old"]: path for path, info in staged_files.items()}

    normalized = set()
    for f in llm_files:
        if not isinstance(f, str):
            continue
        if f in staged_files:
            normalized.add(f)
        elif f in old_to_new:
            normalized.add(old_to_new[f])
    return list(normalized)


def is_file_deleted(path, staged_files):
    """Check if a file is marked as deleted in staged files."""
    info = staged_files.get(path)
    return info["deleted"] if info else False


def is_echoing_keys(commit):
    """Check if commit has literal key strings instead of real values."""
    if not isinstance(commit, dict):
        return True
    files = commit.get("files")
    message = commit.get("message")
    description = commit.get("description")

    # Check if files is invalid
    if files is None:
        return True
    if isinstance(files, str) and files in ("files", "[]", "", "file"):
        return True
    if isinstance(files, list) and len(files) == 0:
        return True

    # Check if message is just echoing the key name
    if message is None:
        return True
    if isinstance(message, str) and message.lower() in ("message", "", "commit message", "msg"):
        return True

    # Check if description is just echoing the key name
    if isinstance(description, str) and description.lower() in ("description", "desc"):
        return True

    return False


def validate_and_fix_commits(commits, staged_files):
    """Validate LLM output and fix issues.

    - Normalizes file paths (handles renames)
    - Removes invalid files
    - Errors if any files are missing from groupings
    - Returns fixed list of commits
    """
    # Detect key-echoing pattern (model failure)
    echo_count = sum(1 for c in commits if is_echoing_keys(c))
    if echo_count > 0 and echo_count == len(commits):
        exit_with_error(
            f"LLM returned malformed commits (echoing keys: {echo_count}/{len(commits)})",
            {"commits_sample": commits[:3]}
        )

    all_staged = set(staged_files.keys())
    covered_files = set()
    fixed_commits = []

    for commit in commits:
        if not isinstance(commit, dict):
            warn(f"skipping non-dict commit: {commit}")
            continue
        if "files" not in commit or "message" not in commit:
            warn(f"skipping malformed commit: {commit}")
            continue

        normalized = normalize_llm_files(commit["files"], staged_files)
        if not normalized:
            warn(f"skipping commit with no valid files: {commit.get('message', '<missing>')}")
            continue

        covered_files.update(normalized)
        fixed_commits.append({
            "files": normalized,
            "message": commit["message"],
            "description": commit.get("description", ""),
        })

    missing_files = all_staged - covered_files
    if missing_files:
        # Add missing files to the last commit or create a new one
        if fixed_commits:
            warn(f"LLM missed {len(missing_files)} files, adding to last commit", {"files": list(missing_files)[:5]})
            fixed_commits[-1]["files"].extend(list(missing_files))
        else:
            # No valid commits at all - create a fallback single commit
            warn("LLM failed to group files, creating single commit")
            fixed_commits.append({
                "files": list(all_staged),
                "message": "chore: update files",
                "description": "",
            })

    return fixed_commits


def get_diff_for_files(files):
    """Get the diff for specific files only."""
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + list(files))
    return strip_diff_noise(diff)


def compress_diff(diff):
    """Compress diff while preserving semantic information.

    Strategy:
    - Include --stat and --dirstat for overview
    - Keep all hunk headers (@@...@@) which show function context
    - Limit lines per hunk, prioritizing additions (+) over context
    - Limit total hunks per file
    """
    excludes = build_pathspec_excludes()
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)
    dirstat, _, _ = run(["git", "diff", "--cached", "--dirstat", "--"] + excludes)

    parts = [f"DIFF STAT:\n{stat}"]
    if dirstat.strip():
        parts.append(f"DIRECTORY CHANGES:\n{dirstat}")
    parts.append("COMPRESSED PATCHES:")

    current_file_header = None
    current_file_lines = []
    hunk_count = 0
    hunk_lines = 0
    hunk_truncated = False

    def flush_file():
        nonlocal current_file_header, current_file_lines, hunk_count
        if current_file_header:
            parts.append(current_file_header)
            parts.extend(current_file_lines)
            if hunk_count > MAX_HUNKS_PER_FILE:
                parts.append(f"[... {hunk_count - MAX_HUNKS_PER_FILE} more hunks truncated]")
        current_file_header = None
        current_file_lines = []
        hunk_count = 0

    for line in diff.splitlines():
        if line.startswith("diff --git"):
            flush_file()
            current_file_header = line
            hunk_count = 0
        elif line.startswith("@@") and " @@" in line:
            hunk_count += 1
            hunk_lines = 0
            hunk_truncated = False
            if hunk_count <= MAX_HUNKS_PER_FILE:
                current_file_lines.append(line)
        elif current_file_header and hunk_count <= MAX_HUNKS_PER_FILE:
            if line.startswith("---") or line.startswith("+++"):
                current_file_lines.append(line)
            elif line.startswith("rename ") or line.startswith("new file") or line.startswith("deleted file"):
                current_file_lines.append(line)
            elif hunk_lines < MAX_LINES_PER_HUNK:
                if line.startswith("+") or line.startswith("-"):
                    current_file_lines.append(line)
                    hunk_lines += 1
                elif line.startswith(" ") and hunk_lines < MAX_LINES_PER_HUNK // 2:
                    current_file_lines.append(line)
                    hunk_lines += 1
            elif not hunk_truncated:
                current_file_lines.append("[... hunk truncated]")
                hunk_truncated = True

    flush_file()
    return "\n".join(parts)


RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}
MAX_RETRIES = 3


class ContextLengthExceeded(Exception):
    """Raised when LLM context length is exceeded."""
    pass


def try_repair_json(content):
    """Attempt to repair truncated or malformed JSON arrays/objects.

    Common issues:
    - Leading garbage: .[{...  or ```json[{...
    - Trailing garbage: }]...  or }]```
    - Truncated mid-string: [..., "file  -> close string and array
    - Truncated mid-object: [..., {"files": ["a"]  -> close object and array
    - Object instead of array: {"files": [...]} when we expected [{...}]
    """
    content = content.strip()

    # Strip leading garbage - find first [ or {
    start_idx = -1
    for i, c in enumerate(content):
        if c in '[{':
            start_idx = i
            break
    if start_idx > 0:
        content = content[start_idx:]

    # Strip trailing garbage - find last ] or }
    end_idx = -1
    for i in range(len(content) - 1, -1, -1):
        if content[i] in ']}':
            end_idx = i
            break
    if end_idx >= 0 and end_idx < len(content) - 1:
        content = content[:end_idx + 1]

    # Try as-is first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try adding closing brackets/braces
    repairs = [
        '"}]',      # truncated mid-string in array
        '"]}}]',    # truncated mid-array in object in array
        '"}]}]',    # truncated mid-string in nested structure
        '"]',       # truncated mid-array
        '"}',       # truncated mid-string in object
        '}]',       # truncated mid-object in array
        ']',        # truncated array
        '}',        # truncated object
        '"]}',      # truncated mid-string in array inside object (phase1 {"files": ["a", "b)
        '"],"hint":""}]',  # phase1 truncated with hint missing
    ]

    for repair in repairs:
        try:
            result = json.loads(content + repair)
            print(f"  repaired truncated JSON with: {repair}")
            return result
        except json.JSONDecodeError:
            continue

    # Try removing the last incomplete element
    # Find the last complete element (ends with }, or ])
    for end_char in ['},', '],', '}', ']', '",']:
        idx = content.rfind(end_char)
        if idx > 0:
            truncated = content[:idx + len(end_char)]
            # Add closing brackets as needed
            for close in [']', ']}', '}]', ']}]', '"]}', '"],"hint":""}]']:
                try:
                    result = json.loads(truncated + close)
                    print(f"  repaired truncated JSON by removing incomplete element")
                    return result
                except json.JSONDecodeError:
                    continue

    return None


def call_llm_raw(config, system_prompt, user_content):
    """Make a raw LLM API call and return parsed JSON."""
    import requests

    use_structured = config.get("STRUCTURED_OUTPUT", "true").lower() == "true"

    request_body = {
        "model": config["MODEL"],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
        "temperature": 0,
        "max_tokens": MAX_OUTPUT_TOKENS,
    }
    if use_structured:
        request_body["response_format"] = {"type": "json_object"}

    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            resp = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config['OPENROUTER_API_KEY']}",
                    "Content-Type": "application/json",
                },
                json=request_body,
                timeout=60,
            )
            if resp.status_code == 200:
                break
            if resp.status_code == 400 and "context length" in resp.text.lower():
                # Context length exceeded - signal caller to use fallback
                log("INFO", "LLM context length exceeded", {"response": resp.text[:300]})
                raise ContextLengthExceeded(resp.text[:500])
            if resp.status_code in RETRYABLE_STATUS_CODES:
                last_error = f"API returned {resp.status_code}: {resp.text[:200]}"
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2**attempt
                    warn(f"LLM API returned {resp.status_code}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                    time.sleep(wait_time)
                    continue
            exit_with_error(
                f"LLM API returned {resp.status_code}",
                {"status_code": resp.status_code, "response": resp.text[:500], "model": config["MODEL"]}
            )
        except requests.exceptions.Timeout:
            last_error = "request timed out"
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request timed out, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request timed out after {MAX_RETRIES} attempts",
                {"model": config["MODEL"], "attempts": MAX_RETRIES}
            )
        except requests.exceptions.RequestException as e:
            last_error = str(e)
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request failed: {e}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request failed after {MAX_RETRIES} attempts: {e}",
                {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": str(e)}
            )
    else:
        exit_with_error(
            f"LLM API failed after {MAX_RETRIES} attempts: {last_error}",
            {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": last_error}
        )
    try:
        resp_json = resp.json()
    except json.JSONDecodeError as e:
        exit_with_error(
            f"failed to parse API response as JSON: {e}",
            {"response_text": resp.text[:1000], "model": config["MODEL"]}
        )
    try:
        content = resp_json["choices"][0]["message"]["content"]
    except (KeyError, IndexError) as e:
        exit_with_error(
            f"unexpected API response structure: {e}",
            {"response_json": str(resp_json)[:1000], "model": config["MODEL"]}
        )
    content = content.strip()
    if content.startswith("```"):
        content = "\n".join(content.split("\n")[1:])
    if content.endswith("```"):
        content = "\n".join(content.split("\n")[:-1])
    content = content.strip()

    # Check if response was truncated (finish_reason)
    finish_reason = resp_json.get("choices", [{}])[0].get("finish_reason", "")
    if finish_reason == "length":
        warn("LLM response was truncated due to length limit")

    # Try normal parse first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try to repair truncated JSON
    repaired = try_repair_json(content)
    if repaired is not None:
        return repaired

    exit_with_error(
        "LLM returned unparseable JSON",
        {"content": content[:1000], "model": config["MODEL"]}
    )


RETRY_PROMPT = """You MUST output REAL file paths and REAL commit messages.

Example of what NOT to do (will be rejected):
[{"files": "files", "message": "message", "description": "description"}]

Example of what you MUST do:
[{"files": ["src/main.go", "src/config.go"], "message": "feat(config): add app settings", "description": "Add runtime configuration support"}]

CRITICAL: Every file in the diff below must appear in the files array.
Output ONLY valid JSON array with no markdown fences:"""


def is_malformed_response(result):
    """Check if response is clearly malformed (key echoing or invalid structure)."""
    if not isinstance(result, list):
        return True
    if not result:
        return True
    # Check if all commits have key-echoing pattern
    echo_count = sum(1 for r in result if is_echoing_keys(r))
    return echo_count > len(result) // 2


def normalize_llm_result(result):
    """Normalize LLM result to a list of commits.

    Handles various response formats:
    - Direct array: [{...}, {...}]
    - Object with commits key: {"commits": [{...}]}
    - Object with groups key: {"groups": [{...}]}
    - Single commit object: {"files": [...], "message": "..."}
    """
    if isinstance(result, list):
        return result
    if isinstance(result, dict):
        if "commits" in result:
            return result["commits"]
        if "groups" in result:
            return result["groups"]
        if "files" in result and "message" in result:
            return [result]
    return []


def call_llm(config, diff):
    """Single-phase: send diff and get commits."""
    content = diff if len(diff) <= MAX_DIFF_CHARS else compress_diff(diff)
    log("INFO", f"sending diff to LLM", {"chars": len(content), "compressed": len(diff) > MAX_DIFF_CHARS})
    result = call_llm_raw(config, SYSTEM_PROMPT, content)
    result = normalize_llm_result(result)

    # Retry with stronger prompt if response looks malformed
    if is_malformed_response(result):
        warn("response looks malformed, retrying with stronger prompt")
        # Use compressed version for retry to save tokens
        retry_content = compress_diff(diff) if len(diff) > MAX_DIFF_CHARS // 2 else content
        result = call_llm_raw(config, RETRY_PROMPT, retry_content)
        result = normalize_llm_result(result)

    return result


def group_files_by_directory(staged_files):
    """Group files by their top-level directory for bulk commits.

    Returns list of groups with files and auto-generated hints.
    """
    from collections import defaultdict
    dir_groups = defaultdict(list)

    for path in staged_files.keys():
        parts = path.split("/")
        if len(parts) > 1:
            top_dir = parts[0]
        else:
            top_dir = "root"
        dir_groups[top_dir].append(path)

    groups = []
    for dir_name, files in sorted(dir_groups.items()):
        groups.append({
            "files": files,
            "hint": f"changes in {dir_name}/"
        })
    return groups


def call_llm_two_phase(config, staged_files):
    """Two-phase approach for large diffs with many files.

    Phase 1: Send file summary (name-status + stat) to get logical groupings
    Phase 2: For each group, send actual diff to generate commit message
    """
    excludes = build_pathspec_excludes()
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M", "--"] + excludes)
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)

    phase1_content = f"FILE CHANGES:\n{name_status}\nSTATISTICS:\n{stat}"

    # Check if phase1 content is too large for the LLM
    if len(phase1_content) > MAX_PHASE1_CHARS:
        warn(f"file summary too large ({len(phase1_content)} chars), grouping by directory instead")
        log("INFO", "skipping LLM phase 1 due to size", {"chars": len(phase1_content)})
        groups = group_files_by_directory(staged_files)
    else:
        log("INFO", "phase 1: sending file summary to LLM", {"chars": len(phase1_content)})
        print("  phase 1: grouping files...")
        try:
            groups = call_llm_raw(config, PHASE1_PROMPT, phase1_content)
        except ContextLengthExceeded:
            warn("LLM context length exceeded, falling back to directory grouping")
            groups = group_files_by_directory(staged_files)

    # Handle case where LLM returns a dict with "groups" key instead of array
    if isinstance(groups, dict):
        groups = groups.get("groups", groups.get("commits", []))

    if not isinstance(groups, list):
        warn(f"phase1 returned non-list ({type(groups).__name__}), falling back to single commit")
        groups = []

    if not groups:
        # Fallback: put all files in one group
        groups = [{"files": list(staged_files.keys()), "hint": "all changes"}]

    commits = []
    for i, group in enumerate(groups):
        if not isinstance(group, dict):
            continue
        files = group.get("files", [])
        if not files:
            continue

        # Normalize files to actual staged paths
        normalized = normalize_llm_files(files, staged_files)
        if not normalized:
            continue

        print(f"  phase 2: generating message for group {i + 1}/{len(groups)}...")
        group_diff = get_diff_for_files(normalized)
        if not group_diff.strip():
            continue
        content = group_diff if len(group_diff) <= MAX_DIFF_CHARS else compress_diff(group_diff)
        log("INFO", f"phase 2 group {i + 1}: sending diff to LLM", {"chars": len(content), "files": len(normalized)})

        try:
            result = call_llm_raw(config, PHASE2_PROMPT, content)
        except ContextLengthExceeded:
            warn(f"context length exceeded for group {i + 1}, using fallback message")
            hint = group.get("hint", "update files")
            commits.append({"files": normalized, "message": f"chore: {hint}", "description": ""})
            continue

        # Handle various response formats - extract dict from list if needed
        if isinstance(result, list) and len(result) > 0:
            result = result[0]
        if not isinstance(result, dict):
            warn(f"phase2 returned non-dict for group {i + 1}, skipping")
            continue
        message = result.get("message")
        if not message or message.lower() in ("message", ""):
            warn(f"phase2 missing message for group {i + 1}, using fallback")
            message = "chore: update files"
        description = result.get("description", "")
        if description.lower() in ("description", "desc"):
            description = ""

        commits.append({"files": normalized, "message": message, "description": description})

    return commits


def get_lockfiles_in_staging():
    """Get list of lockfiles that are staged."""
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    return [f for f in staged.strip().splitlines() if f and is_lockfile(f)]


def main():
    parser = argparse.ArgumentParser(description="Fast commit with LLM-generated messages")
    parser.add_argument("--no-verify", action="store_true", help="bypass pre-commit hooks")
    parser.add_argument("--dry-run", action="store_true", help="preview commits without executing")
    parser.add_argument("--no-push", action="store_true", help="skip automatic push")
    parser.add_argument("--bulk", action="store_true", help="skip LLM and group by directory (for large batches)")
    args = parser.parse_args()

    log("INFO", "session started", {"dry_run": args.dry_run, "no_verify": args.no_verify, "no_push": args.no_push, "bulk": args.bulk})
    log_git_state()

    config = load_config()
    log("INFO", f"using model: {config['MODEL']}")

    diff, source = get_diff()
    if not diff:
        print("nothing to commit")
        return

    staged_files = get_staged_files_for_commit()
    file_count = len(staged_files)

    if file_count == 0:
        print("nothing to commit")
        return

    # Check for lockfiles before we reset staging (must happen before reset)
    lockfiles = get_lockfiles_in_staging() if source == "all" else []

    print(f"analysing {source} changes ({file_count} files)...")

    # For very large file counts or explicit --bulk flag, skip LLM entirely
    if args.bulk or file_count > BULK_FILE_THRESHOLD:
        if args.bulk:
            print("  using bulk mode (--bulk flag)")
            log("INFO", "using bulk mode (--bulk flag)", {"file_count": file_count})
        else:
            warn(f"file count ({file_count}) exceeds bulk threshold ({BULK_FILE_THRESHOLD}), using directory grouping")
            log("INFO", "using bulk mode (auto)", {"file_count": file_count})
        groups = group_files_by_directory(staged_files)
        # Generate simple commit messages based on directory groups
        commits = []
        for group in groups:
            files = group["files"]
            hint = group.get("hint", "update files")
            # Simple message based on operation type
            sample_info = staged_files.get(files[0], {})
            if sample_info.get("deleted"):
                msg = f"chore: remove {hint.replace('changes in ', '')}"
            else:
                msg = f"chore: {hint}"
            commits.append({
                "files": files,
                "message": msg,
                "description": "",
            })
    elif file_count > LARGE_DIFF_FILES:
        print("  using two-phase approach for large diff")
        commits = call_llm_two_phase(config, staged_files)
        commits = validate_and_fix_commits(commits, staged_files)
    else:
        commits = call_llm(config, diff)
        commits = validate_and_fix_commits(commits, staged_files)

    if not commits:
        exit_with_error("no valid commits generated", {"file_count": file_count})

    if args.dry_run:
        print("\ndry run - would create these commits:")
        for commit in commits:
            print(f"  {commit['message']}")
            if commit.get("description"):
                print(f"    {commit['description']}")
            for f in commit["files"]:
                print(f"    - {f}")
        if lockfiles:
            print(f"  chore: update lockfiles")
            for f in lockfiles:
                print(f"    - {f}")
        return

    if source == "all":
        run(["git", "reset", "HEAD"], check=True)

    commit_args = ["git", "commit"]
    if args.no_verify:
        commit_args.append("--no-verify")

    for commit in commits:
        files = commit["files"]
        message = commit["message"]
        description = commit.get("description", "")

        # Stage files - use git rm for deleted files, git add for others
        added_files = []
        for f in files:
            if is_file_deleted(f, staged_files):
                _, stderr, code = run(["git", "rm", "--", f])
            else:
                _, stderr, code = run(["git", "add", "--", f])
            if code == 0:
                added_files.append(f)
            else:
                warn(f"could not stage {f}: {stderr.strip()}")

        if not added_files:
            print(f"  skipping commit (no files staged): {message}")
            continue

        # Use two -m flags: first for subject, second for body
        commit_cmd = commit_args + ["-m", message]
        if description:
            commit_cmd += ["-m", description]
        _, stderr, code = run(commit_cmd)
        if code != 0:
            # Detect hook failures
            if "hook" in stderr.lower() or "pre-commit" in stderr.lower():
                exit_with_error(
                    f"commit blocked by git hook. Fix the issues or use --no-verify to bypass",
                    {"message": message, "files": added_files, "stderr": stderr}
                )
            exit_with_error(
                f"commit failed: {stderr}",
                {"message": message, "files": added_files, "stderr": stderr}
            )
        log("INFO", f"committed: {message}", {"files": len(added_files)})
        print(f"  {message}")

    # Commit lockfiles separately with a simple message
    if lockfiles:
        run(["git", "add", "--"] + lockfiles, check=True)
        # Verify something is actually staged before committing
        staged_check, _, _ = run(["git", "diff", "--cached", "--name-only"])
        if staged_check.strip():
            _, stderr, code = run(commit_args + ["-m", "chore: update lockfiles"])
            if code != 0:
                if "hook" in stderr.lower():
                    exit_with_error(
                        f"lockfile commit blocked by git hook. Use --no-verify to bypass",
                        {"lockfiles": lockfiles, "stderr": stderr}
                    )
                exit_with_error(
                    f"lockfile commit failed: {stderr}",
                    {"lockfiles": lockfiles, "stderr": stderr}
                )
            log("INFO", "committed lockfiles", {"count": len(lockfiles)})
            print("  chore: update lockfiles")

    if args.no_push:
        log("INFO", "session completed (push skipped)")
        print("done (push skipped)")
        return

    _, stderr, code = run(["git", "push"])
    if code != 0:
        # Detect out-of-sync (rejected push)
        if "rejected" in stderr.lower() or "non-fast-forward" in stderr.lower():
            exit_with_error(
                "push rejected: remote has changes. Run 'git pull' first to sync",
                {"stderr": stderr}
            )
        if "no upstream" in stderr.lower() or "no configured push destination" in stderr.lower():
            exit_with_error(
                "push failed: no upstream branch. Run 'git push -u origin <branch>' first",
                {"stderr": stderr}
            )
        exit_with_error("push failed", {"stderr": stderr})
    log("INFO", "pushed to remote")
    log("INFO", "session completed successfully")
    print("pushed")


if __name__ == "__main__":
    main()

