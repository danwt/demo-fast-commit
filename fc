#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
# ]
# ///

import argparse
import json
import subprocess
import sys
import threading
import time
import traceback
from datetime import datetime
from pathlib import Path

CONFIG_PATH = Path.home() / ".config" / "fast-commit" / ".env"
LOGS_DIR = Path.home() / ".fastc" / "logs"
AUDIT_LOG = Path.home() / ".fastc" / "audit.jsonl"

# Session log file - created once per run
_session_log = None


def get_session_log():
    """Get or create session log file for this execution."""
    global _session_log
    if _session_log is None:
        LOGS_DIR.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        _session_log = LOGS_DIR / f"session_{timestamp}.log"
        _session_log.write_text(f"=== fastc session started: {datetime.now().isoformat()} ===\n")
    return _session_log


def log(level, message, context=None):
    """Log a message to the session log file."""
    log_file = get_session_log()
    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] {level}: {message}"
    if context:
        context_str = ", ".join(f"{k}={v}" for k, v in context.items())
        line += f" | {context_str}"
    with open(log_file, "a") as f:
        f.write(line + "\n")


MAX_STATUS_LINES_IN_LOG = 100


def log_git_state():
    """Log current git state to session log."""
    try:
        branch_result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True, timeout=5
        )
        branch = branch_result.stdout.decode().strip() if branch_result.returncode == 0 else "unknown"
        log("INFO", f"git branch: {branch}")

        status_result = subprocess.run(
            ["git", "status", "--short"],
            capture_output=True, timeout=5
        )
        if status_result.returncode == 0:
            status = status_result.stdout.decode().strip()
            if status:
                lines = status.splitlines()
                if len(lines) > MAX_STATUS_LINES_IN_LOG:
                    truncated = "\n".join(lines[:MAX_STATUS_LINES_IN_LOG])
                    log("INFO", f"git status (truncated, {len(lines)} files total):\n{truncated}\n... and {len(lines) - MAX_STATUS_LINES_IN_LOG} more")
                else:
                    log("INFO", f"git status:\n{status}")
    except Exception as e:
        log("INFO", f"failed to capture git state: {e}")


# ---------------------------------------------------------------------------
# Spinner + output helpers
# ---------------------------------------------------------------------------

TICK = "✓"
CROSS = "✗"
WARN_SYM = "⚠"
SPINNER_FRAMES = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]

_output_lock = threading.Lock()


class Spinner:
    """Thread-based terminal spinner. All output must go through this object
    so that the spinner line is cleared before any print."""

    def __init__(self):
        self._msg = ""
        self._stop = threading.Event()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def _run(self):
        i = 0
        while not self._stop.wait(0.08):
            with _output_lock:
                frame = SPINNER_FRAMES[i % len(SPINNER_FRAMES)]
                sys.stdout.write(f"\r{frame} {self._msg}")
                sys.stdout.flush()
            i += 1

    def set(self, message):
        """Update the spinner message."""
        self._msg = message

    def _clear(self):
        """Clear spinner line. Must be called with _output_lock held."""
        sys.stdout.write("\r" + " " * (len(self._msg) + 4) + "\r")
        sys.stdout.flush()

    def println(self, message, file=sys.stdout):
        """Print a line, temporarily pausing the spinner."""
        with _output_lock:
            self._clear()
            print(message, file=file)

    def ok(self, message):
        self.println(f"{TICK} {message}")

    def fail(self, message):
        self.println(f"{CROSS} {message}", file=sys.stderr)

    def warn(self, message):
        self.println(f"{WARN_SYM} {message}", file=sys.stderr)

    def stop(self):
        self._stop.set()
        self._thread.join()
        with _output_lock:
            self._clear()


_spinner: "Spinner | None" = None


def sprint(message, file=sys.stdout):
    """Print a message, clearing the spinner line first if one is active."""
    global _spinner
    if _spinner:
        _spinner.println(message, file=file)
    else:
        print(message, file=file)


# ---------------------------------------------------------------------------
# Error / warning helpers
# ---------------------------------------------------------------------------

def warn(message, context=None):
    """Print warning to user and log it."""
    global _spinner
    if _spinner:
        _spinner.warn(message)
    else:
        print(f"{WARN_SYM} {message}", file=sys.stderr)
    log("INFO", message, context)


def record_error(message, context=None):
    """Record error details to session log with full context."""
    log("ERROR", message, context)
    exc_info = traceback.format_exc()
    if exc_info and exc_info.strip() != "NoneType: None":
        log_file = get_session_log()
        with open(log_file, "a") as f:
            f.write(f"Stack trace:\n{exc_info}\n")
    log_git_state()
    sprint(f"  session log: {get_session_log()}", file=sys.stderr)


_audit_repo = None


def audit(event, data=None):
    """Append a structured entry to the persistent audit log."""
    global _audit_repo
    AUDIT_LOG.parent.mkdir(parents=True, exist_ok=True)
    if _audit_repo is None:
        repo_result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True, timeout=5,
        )
        _audit_repo = repo_result.stdout.decode().strip() if repo_result.returncode == 0 else "unknown"
    entry = {
        "ts": datetime.now().isoformat(),
        "repo": _audit_repo,
        "event": event,
    }
    if data:
        entry["data"] = data
    with open(AUDIT_LOG, "a") as f:
        f.write(json.dumps(entry) + "\n")


def exit_with_error(message, context=None):
    """Print error, record to disk, and exit."""
    global _spinner
    if _spinner:
        _spinner.fail(f"error: {message}")
        _spinner.stop()
    else:
        print(f"error: {message}", file=sys.stderr)
    record_error(message, context)
    audit("error", {"message": message})
    sys.exit(1)


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

# Files to exclude from diff analysis (don't help with commit messages)
EXCLUDED_PATTERNS = [
    # JS/Node lockfiles
    "pnpm-lock.yaml",
    "package-lock.json",
    "yarn.lock",
    "bun.lockb",
    # Other language lockfiles
    "Cargo.lock",
    "poetry.lock",
    "Pipfile.lock",
    "go.sum",
    "composer.lock",
    "Gemfile.lock",
    # Minified/generated files
    "*.min.js",
    "*.min.css",
    "*.map",
]

SYSTEM_PROMPT = """You are a git commit message generator. Given a git diff, analyse the changes and group them into logical atomic commits.

CRITICAL REQUIREMENTS:
- "files" MUST be actual file paths from the diff (e.g., "src/auth.go", "tests/auth_test.go")
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"
- Every file in the diff MUST appear in exactly one commit's files array

Rules:
- Each commit should represent one logical change
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single commit

BAD OUTPUT (will be rejected):
[
  {"files": "files", "message": "message", "description": "description"}
]

GOOD OUTPUT:
[
  {
    "files": ["src/auth.go", "tests/auth_test.go"],
    "message": "feat(auth): add login endpoint",
    "description": "Users need to authenticate before accessing protected resources."
  }
]

Respond with ONLY a valid JSON array (no markdown, no explanation):"""

PHASE1_PROMPT = """You are a git commit analyzer. Given a summary of changed files (name-status and stats), group them into logical atomic commits.

CRITICAL REQUIREMENT:
- "files" MUST be actual file paths from the FILE CHANGES list below

Rules:
- Each group should represent one logical change
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single group
- Use the file paths and change types (A=added, M=modified, D=deleted, R=renamed) to understand the changes

BAD OUTPUT (will be rejected):
[{"files": "files", "hint": "hint"}]

GOOD OUTPUT:
[{"files": ["src/auth.go", "tests/auth_test.go"], "hint": "authentication logic changes"}]

Respond with ONLY a valid JSON array of file groups (no markdown, no explanation):"""

PHASE2_PROMPT = """You are a git commit message generator. Given a diff for a specific group of related files, generate ONE commit message.

CRITICAL REQUIREMENTS:
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"

Rules:
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed

BAD OUTPUT (will be rejected):
{"message": "message", "description": "description"}

GOOD OUTPUT:
{"message": "feat(auth): add login endpoint", "description": "Users need to authenticate before accessing protected resources."}

Respond with ONLY a valid JSON object (no markdown, no explanation):"""


# ---------------------------------------------------------------------------
# Config + git helpers
# ---------------------------------------------------------------------------

def load_config():
    if not CONFIG_PATH.exists():
        exit_with_error(
            f"config not found at {CONFIG_PATH}. Create it with OPENROUTER_API_KEY and MODEL",
            {"config_path": str(CONFIG_PATH)}
        )
    config = {}
    for line in CONFIG_PATH.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        key, _, value = line.partition("=")
        config[key.strip()] = value.strip().strip("\"'")
    if "OPENROUTER_API_KEY" not in config:
        exit_with_error("OPENROUTER_API_KEY not set in config", {"config_path": str(CONFIG_PATH)})
    if "MODEL" not in config:
        exit_with_error("MODEL not set in config", {"config_path": str(CONFIG_PATH)})
    return config


def run(cmd, check=False, timeout=60):
    try:
        result = subprocess.run(cmd, capture_output=True, timeout=timeout)
    except subprocess.TimeoutExpired:
        exit_with_error(
            f"command timed out after {timeout}s: {' '.join(cmd)}",
            {"command": " ".join(cmd), "timeout": timeout}
        )
    stdout = result.stdout.decode("utf-8", errors="replace")
    stderr = result.stderr.decode("utf-8", errors="replace")
    if check and result.returncode != 0:
        exit_with_error(
            f"command failed: {' '.join(cmd)}",
            {"command": " ".join(cmd), "stderr": stderr.strip(), "returncode": result.returncode}
        )
    return stdout, stderr, result.returncode


def build_pathspec_excludes():
    """Build git pathspec exclude args for lockfiles etc."""
    excludes = []
    for pattern in EXCLUDED_PATTERNS:
        excludes.append(":(exclude)**/" + pattern)
    return excludes


def strip_diff_noise(diff):
    """Remove noise lines from diff to save tokens."""
    lines = []
    for line in diff.splitlines():
        if line.startswith("index "):
            continue
        if line.startswith("similarity index "):
            continue
        if line.startswith("dissimilarity index "):
            continue
        lines.append(line)
    return "\n".join(lines)


def get_diff():
    """Get diff for analysis, excluding lockfiles from LLM context."""
    excludes = build_pathspec_excludes()
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    if staged.strip():
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        diff = strip_diff_noise(diff)
        if diff:
            return diff, "staged"
        # Staged files were all lockfiles/excluded — fall through to check unstaged
    diff, _, _ = run(["git", "diff", "--minimal"])
    if not diff.strip():
        untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
        if not untracked.strip():
            return "", "none"
        run(["git", "add", "-A"], check=True)
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "all"
    run(["git", "add", "-A"], check=True)
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
    return strip_diff_noise(diff), "all"


MAX_DIFF_CHARS = 20000
MAX_LINES_PER_HUNK = 15
MAX_HUNKS_PER_FILE = 5
LARGE_DIFF_FILES = 15
MAX_OUTPUT_TOKENS = 4096
# Beyond this many files, skip LLM entirely and use bulk commit
BULK_FILE_THRESHOLD = 200
# Maximum characters for phase 1 file summary before fallback
MAX_PHASE1_CHARS = 50000
# LLM retry attempts when files are missed
MAX_LLM_RETRIES = 3


def is_lockfile(path):
    """Check if a file path matches any lockfile pattern."""
    for pattern in EXCLUDED_PATTERNS:
        if pattern.startswith("*"):
            if path.endswith(pattern[1:]):
                return True
        elif path.endswith("/" + pattern) or path == pattern:
            return True
    return False


def git_unquote(path):
    """Decode git's C-style quoted path."""
    if not (path.startswith('"') and path.endswith('"')):
        return path
    inner = path[1:-1]
    result = bytearray()
    i = 0
    while i < len(inner):
        if inner[i] == '\\' and i + 1 < len(inner):
            c = inner[i + 1]
            if c in '01234567' and i + 3 < len(inner):
                result.append(int(inner[i + 1:i + 4], 8))
                i += 4
            elif c == '\\':
                result.append(ord('\\'))
                i += 2
            elif c == '"':
                result.append(ord('"'))
                i += 2
            elif c == 'n':
                result.append(ord('\n'))
                i += 2
            elif c == 't':
                result.append(ord('\t'))
                i += 2
            else:
                result.extend(inner[i].encode('utf-8'))
                i += 1
        else:
            result.extend(inner[i].encode('utf-8'))
            i += 1
    return result.decode('utf-8')


def get_staged_files_for_commit():
    """Get actual staged files that can be committed (excluding lockfiles).

    Returns dict mapping path -> {old_path, is_deleted}
    """
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M"])
    files = {}
    for line in name_status.strip().splitlines():
        if not line:
            continue
        parts = line.split("\t")
        status = parts[0]
        if status.startswith("R"):  # Rename: R100\told\tnew
            old_path = git_unquote(parts[1])
            new_path = git_unquote(parts[2])
            if not is_lockfile(new_path):
                files[new_path] = {"old": old_path, "deleted": False}
        elif status == "D":  # Deleted
            path = git_unquote(parts[1])
            if not is_lockfile(path):
                files[path] = {"old": path, "deleted": True}
        else:  # Added, Modified, etc
            path = git_unquote(parts[1])
            if not is_lockfile(path):
                files[path] = {"old": path, "deleted": False}
    return files


def normalize_llm_files(llm_files, staged_files):
    """Map LLM file paths to actual staged paths."""
    if not isinstance(llm_files, list):
        return []
    old_to_new = {info["old"]: path for path, info in staged_files.items()}
    normalized = set()
    for f in llm_files:
        if not isinstance(f, str):
            continue
        if f in staged_files:
            normalized.add(f)
        elif f in old_to_new:
            normalized.add(old_to_new[f])
    return list(normalized)


def is_file_deleted(path, staged_files):
    """Check if a file is marked as deleted in staged files."""
    info = staged_files.get(path)
    return info["deleted"] if info else False


def is_echoing_keys(commit):
    """Check if commit has literal key strings instead of real values."""
    if not isinstance(commit, dict):
        return True
    files = commit.get("files")
    message = commit.get("message")
    description = commit.get("description")

    if files is None:
        return True
    if isinstance(files, str) and files in ("files", "[]", "", "file"):
        return True
    if isinstance(files, list) and len(files) == 0:
        return True

    if message is None:
        return True
    if isinstance(message, str) and message.lower() in ("message", "", "commit message", "msg"):
        return True

    if isinstance(description, str) and description.lower() in ("description", "desc"):
        return True

    return False


def validate_and_fix_commits(commits, staged_files):
    """Validate LLM output and fix issues.

    Returns (fixed_commits, missing_files_set).
    missing_files_set is empty when all staged files are covered.
    """
    # Detect key-echoing pattern (model failure)
    echo_count = sum(1 for c in commits if is_echoing_keys(c))
    if echo_count > 0 and echo_count == len(commits):
        exit_with_error(
            f"LLM returned malformed commits (echoing keys: {echo_count}/{len(commits)})",
            {"commits_sample": commits[:3]}
        )

    all_staged = set(staged_files.keys())
    covered_files = set()
    fixed_commits = []

    for commit in commits:
        if not isinstance(commit, dict):
            warn(f"skipping non-dict commit: {commit}")
            continue
        if "files" not in commit or "message" not in commit:
            warn(f"skipping malformed commit: {commit}")
            continue

        normalized = normalize_llm_files(commit["files"], staged_files)
        if not normalized:
            warn(f"skipping commit with no valid files: {commit.get('message', '<missing>')}")
            continue

        covered_files.update(normalized)
        fixed_commits.append({
            "files": normalized,
            "message": commit["message"],
            "description": commit.get("description", ""),
        })

    if not fixed_commits:
        # No valid commits at all - create a fallback single commit
        warn("LLM failed to group files, creating single commit")
        return [{
            "files": list(all_staged),
            "message": "chore: update files",
            "description": "",
        }], set()

    missing_files = all_staged - covered_files
    return fixed_commits, missing_files


def get_diff_for_files(files):
    """Get the diff for specific files only."""
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + list(files))
    return strip_diff_noise(diff)


def compress_diff(diff):
    """Compress diff while preserving semantic information."""
    excludes = build_pathspec_excludes()
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)
    dirstat, _, _ = run(["git", "diff", "--cached", "--dirstat", "--"] + excludes)

    parts = [f"DIFF STAT:\n{stat}"]
    if dirstat.strip():
        parts.append(f"DIRECTORY CHANGES:\n{dirstat}")
    parts.append("COMPRESSED PATCHES:")

    current_file_header = None
    current_file_lines = []
    hunk_count = 0
    hunk_lines = 0
    hunk_truncated = False

    def flush_file():
        nonlocal current_file_header, current_file_lines, hunk_count
        if current_file_header:
            parts.append(current_file_header)
            parts.extend(current_file_lines)
            if hunk_count > MAX_HUNKS_PER_FILE:
                parts.append(f"[... {hunk_count - MAX_HUNKS_PER_FILE} more hunks truncated]")
        current_file_header = None
        current_file_lines = []
        hunk_count = 0

    for line in diff.splitlines():
        if line.startswith("diff --git"):
            flush_file()
            current_file_header = line
            hunk_count = 0
        elif line.startswith("@@") and " @@" in line:
            hunk_count += 1
            hunk_lines = 0
            hunk_truncated = False
            if hunk_count <= MAX_HUNKS_PER_FILE:
                current_file_lines.append(line)
        elif current_file_header and hunk_count <= MAX_HUNKS_PER_FILE:
            if line.startswith("---") or line.startswith("+++"):
                current_file_lines.append(line)
            elif line.startswith("rename ") or line.startswith("new file") or line.startswith("deleted file"):
                current_file_lines.append(line)
            elif hunk_lines < MAX_LINES_PER_HUNK:
                if line.startswith("+") or line.startswith("-"):
                    current_file_lines.append(line)
                    hunk_lines += 1
                elif line.startswith(" ") and hunk_lines < MAX_LINES_PER_HUNK // 2:
                    current_file_lines.append(line)
                    hunk_lines += 1
            elif not hunk_truncated:
                current_file_lines.append("[... hunk truncated]")
                hunk_truncated = True

    flush_file()
    return "\n".join(parts)


RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}
MAX_RETRIES = 3


class ContextLengthExceeded(Exception):
    """Raised when LLM context length is exceeded."""
    pass


def try_repair_json(content):
    """Attempt to repair truncated or malformed JSON arrays/objects."""
    content = content.strip()

    # Strip leading garbage - find first [ or {
    start_idx = -1
    for i, c in enumerate(content):
        if c in '[{':
            start_idx = i
            break
    if start_idx > 0:
        content = content[start_idx:]

    # Strip trailing garbage - find last ] or }
    end_idx = -1
    for i in range(len(content) - 1, -1, -1):
        if content[i] in ']}':
            end_idx = i
            break
    if end_idx >= 0 and end_idx < len(content) - 1:
        content = content[:end_idx + 1]

    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    repairs = [
        '"}]',
        '"]}}]',
        '"}]}]',
        '"]',
        '"}',
        '}]',
        ']',
        '}',
        '"]}',
        '"],"hint":""}]',
    ]

    for repair in repairs:
        try:
            result = json.loads(content + repair)
            warn(f"repaired truncated JSON (appended: {repair})")
            return result
        except json.JSONDecodeError:
            continue

    for end_char in ['},', '],', '}', ']', '",']:
        idx = content.rfind(end_char)
        if idx > 0:
            truncated = content[:idx + len(end_char)]
            for close in [']', ']}', '}]', ']}]', '"]}', '"],"hint":""}]']:
                try:
                    result = json.loads(truncated + close)
                    warn("repaired truncated JSON by removing incomplete element")
                    return result
                except json.JSONDecodeError:
                    continue

    return None


def call_llm_raw(config, system_prompt, user_content):
    """Make a raw LLM API call and return parsed JSON."""
    import requests

    use_structured = config.get("STRUCTURED_OUTPUT", "true").lower() == "true"

    request_body = {
        "model": config["MODEL"],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
        "temperature": 0,
        "max_tokens": MAX_OUTPUT_TOKENS,
    }
    if use_structured:
        request_body["response_format"] = {"type": "json_object"}

    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            resp = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config['OPENROUTER_API_KEY']}",
                    "Content-Type": "application/json",
                },
                json=request_body,
                timeout=60,
            )
            if resp.status_code == 200:
                break
            if resp.status_code == 400 and "context length" in resp.text.lower():
                log("INFO", "LLM context length exceeded", {"response": resp.text[:300]})
                raise ContextLengthExceeded(resp.text[:500])
            if resp.status_code in RETRYABLE_STATUS_CODES:
                last_error = f"API returned {resp.status_code}: {resp.text[:200]}"
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2**attempt
                    warn(f"LLM API returned {resp.status_code}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                    time.sleep(wait_time)
                    continue
            exit_with_error(
                f"LLM API returned {resp.status_code}",
                {"status_code": resp.status_code, "response": resp.text[:500], "model": config["MODEL"]}
            )
        except requests.exceptions.Timeout:
            last_error = "request timed out"
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request timed out, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request timed out after {MAX_RETRIES} attempts",
                {"model": config["MODEL"], "attempts": MAX_RETRIES}
            )
        except requests.exceptions.RequestException as e:
            last_error = str(e)
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request failed: {e}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request failed after {MAX_RETRIES} attempts: {e}",
                {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": str(e)}
            )
    try:
        resp_json = resp.json()
    except json.JSONDecodeError as e:
        exit_with_error(
            f"failed to parse API response as JSON: {e}",
            {"response_text": resp.text[:1000], "model": config["MODEL"]}
        )
    try:
        content = resp_json["choices"][0]["message"]["content"]
    except (KeyError, IndexError) as e:
        exit_with_error(
            f"unexpected API response structure: {e}",
            {"response_json": str(resp_json)[:1000], "model": config["MODEL"]}
        )
    content = content.strip()
    if content.startswith("```"):
        content = "\n".join(content.split("\n")[1:])
    if content.endswith("```"):
        content = "\n".join(content.split("\n")[:-1])
    content = content.strip()

    finish_reason = resp_json.get("choices", [{}])[0].get("finish_reason", "")
    if finish_reason == "length":
        warn("LLM response was truncated due to length limit")

    if not content:
        record_error("LLM returned empty response", {"model": config["MODEL"]})
        return []

    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    repaired = try_repair_json(content)
    if repaired is not None:
        return repaired

    record_error(
        "LLM returned unparseable JSON",
        {"content": content[:1000], "model": config["MODEL"]}
    )
    return []


RETRY_PROMPT = """You MUST output REAL file paths and REAL commit messages.

Example of what NOT to do (will be rejected):
[{"files": "files", "message": "message", "description": "description"}]

Example of what you MUST do:
[{"files": ["src/main.go", "src/config.go"], "message": "feat(config): add app settings", "description": "Add runtime configuration support"}]

CRITICAL: Every file in the diff below must appear in the files array.
Output ONLY valid JSON array with no markdown fences:"""


def is_malformed_response(result):
    """Check if response is clearly malformed (key echoing or invalid structure)."""
    if not isinstance(result, list):
        return True
    if not result:
        return True
    echo_count = sum(1 for r in result if is_echoing_keys(r))
    return echo_count > len(result) // 2


def normalize_llm_result(result):
    """Normalize LLM result to a list of commits."""
    if isinstance(result, list):
        return result
    if isinstance(result, dict):
        if "commits" in result:
            return result["commits"]
        if "groups" in result:
            return result["groups"]
        if "files" in result and "message" in result:
            return [result]
    return []


def call_llm(config, diff, missing_hint=None):
    """Single-phase: send diff and get commits.

    missing_hint: set of file paths the LLM missed on a previous attempt.
    """
    content = diff if len(diff) <= MAX_DIFF_CHARS else compress_diff(diff)
    if missing_hint:
        hint_line = "IMPORTANT: You previously missed these files — they MUST be included: " + ", ".join(sorted(missing_hint))
        content = hint_line + "\n\n" + content
    log("INFO", "sending diff to LLM", {"chars": len(content), "compressed": len(diff) > MAX_DIFF_CHARS})
    result = call_llm_raw(config, SYSTEM_PROMPT, content)
    result = normalize_llm_result(result)

    if is_malformed_response(result):
        warn("response looks malformed, retrying with stronger prompt")
        retry_content = compress_diff(diff) if len(diff) > MAX_DIFF_CHARS // 2 else content
        result = call_llm_raw(config, RETRY_PROMPT, retry_content)
        result = normalize_llm_result(result)

    return result


def detect_directory_moves(staged_files):
    """Detect directory-level rename/move operations."""
    from collections import defaultdict

    transforms = defaultdict(list)

    for path, info in staged_files.items():
        old_path = info.get("old", path)
        if old_path == path or info.get("deleted"):
            continue

        old_parts = old_path.split("/")
        new_parts = path.split("/")

        common_suffix_len = 0
        for i in range(1, min(len(old_parts), len(new_parts)) + 1):
            if old_parts[-i] == new_parts[-i]:
                common_suffix_len = i
            else:
                break

        if common_suffix_len > 0:
            old_prefix = "/".join(old_parts[:-common_suffix_len])
            new_prefix = "/".join(new_parts[:-common_suffix_len])
        else:
            old_prefix = "/".join(old_parts[:-1])
            new_prefix = "/".join(new_parts[:-1])

        transforms[(old_prefix, new_prefix)].append(path)

    return {
        new_prefix: (old_prefix, files)
        for (old_prefix, new_prefix), files in transforms.items()
        if len(files) >= 3
    }


def summarize_file_operations(staged_files):
    """Analyze staged files and return high-level operation summaries."""
    from collections import defaultdict

    operations = []
    handled_files = set()

    dir_moves = detect_directory_moves(staged_files)
    for new_prefix, (old_prefix, files) in sorted(dir_moves.items(), key=lambda x: -len(x[1])):
        files = [f for f in files if f not in handled_files]
        if len(files) < 3:
            continue

        handled_files.update(files)
        old_name = old_prefix.split("/")[-1] if old_prefix else "files"
        new_name = new_prefix.split("/")[-1] if new_prefix else "root"

        if old_prefix and new_prefix:
            if old_name == new_name:
                msg = f"refactor: move {old_prefix}/ to {new_prefix}/"
            else:
                msg = f"refactor: rename {old_prefix}/ to {new_prefix}/"
        elif new_prefix:
            msg = f"refactor: move files into {new_prefix}/"
        else:
            msg = f"refactor: move {old_prefix}/ to root"

        operations.append({
            "files": files,
            "message": msg,
            "description": f"{len(files)} files moved"
        })

    remaining = {p: info for p, info in staged_files.items() if p not in handled_files}

    by_op_and_dir = defaultdict(lambda: defaultdict(list))
    for path, info in remaining.items():
        if info.get("deleted"):
            op = "delete"
        elif info.get("old") != path:
            op = "rename"
        else:
            op = "add"

        parts = path.split("/")
        if len(parts) > 2:
            dir_key = "/".join(parts[:2])
        elif len(parts) > 1:
            dir_key = parts[0]
        else:
            dir_key = "root"

        by_op_and_dir[op][dir_key].append(path)

    for dir_key, files in by_op_and_dir["delete"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"chore: remove {dir_key}/",
                "description": f"{len(files)} files deleted"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"chore: remove {fname}",
                    "description": ""
                })
                handled_files.add(f)

    for dir_key, files in by_op_and_dir["add"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"feat: add {dir_key}/",
                "description": f"{len(files)} files added"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"feat: add {fname}",
                    "description": ""
                })
                handled_files.add(f)

    for dir_key, files in by_op_and_dir["rename"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"refactor: rename files in {dir_key}/",
                "description": f"{len(files)} files renamed"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"refactor: rename {fname}",
                    "description": ""
                })
                handled_files.add(f)

    return operations


def group_files_by_directory(staged_files):
    """Smart grouping that detects directory-level operations."""
    operations = summarize_file_operations(staged_files)
    if operations:
        return operations
    return [{
        "files": list(staged_files.keys()),
        "message": "chore: update files",
        "description": f"{len(staged_files)} files changed"
    }]


def call_llm_two_phase(config, staged_files, missing_hint=None):
    """Two-phase approach for large diffs with many files."""
    global _spinner

    excludes = build_pathspec_excludes()
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M", "--"] + excludes)
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)

    phase1_content = f"FILE CHANGES:\n{name_status}\nSTATISTICS:\n{stat}"
    if missing_hint:
        hint_line = "IMPORTANT: You previously missed these files — include ALL of them: " + ", ".join(sorted(missing_hint))
        phase1_content = hint_line + "\n\n" + phase1_content

    if len(phase1_content) > MAX_PHASE1_CHARS:
        warn(f"file summary too large ({len(phase1_content)} chars), grouping by directory instead")
        log("INFO", "skipping LLM phase 1 due to size", {"chars": len(phase1_content)})
        groups = group_files_by_directory(staged_files)
    else:
        log("INFO", "phase 1: sending file summary to LLM", {"chars": len(phase1_content)})
        if _spinner:
            _spinner.set("Grouping files...")
        try:
            groups = call_llm_raw(config, PHASE1_PROMPT, phase1_content)
        except ContextLengthExceeded:
            warn("LLM context length exceeded, falling back to directory grouping")
            groups = group_files_by_directory(staged_files)

    if isinstance(groups, dict):
        groups = groups.get("groups", groups.get("commits", []))

    if not isinstance(groups, list):
        warn(f"phase 1 returned non-list ({type(groups).__name__}), using smart grouping")
        groups = group_files_by_directory(staged_files)

    if not groups:
        groups = group_files_by_directory(staged_files)

    commits = []
    for i, group in enumerate(groups):
        if not isinstance(group, dict):
            continue
        files = group.get("files", [])
        if not files:
            continue

        if group.get("message"):
            normalized = normalize_llm_files(files, staged_files) if isinstance(files[0], str) else files
            if normalized:
                commits.append({
                    "files": normalized,
                    "message": group["message"],
                    "description": group.get("description", ""),
                })
            continue

        normalized = normalize_llm_files(files, staged_files)
        if not normalized:
            continue

        if _spinner:
            _spinner.set(f"Writing message {i + 1}/{len(groups)}...")
        group_diff = get_diff_for_files(normalized)
        if not group_diff.strip():
            continue
        content = group_diff if len(group_diff) <= MAX_DIFF_CHARS else compress_diff(group_diff)
        log("INFO", f"phase 2 group {i + 1}: sending diff to LLM", {"chars": len(content), "files": len(normalized)})

        try:
            result = call_llm_raw(config, PHASE2_PROMPT, content)
        except ContextLengthExceeded:
            warn(f"context length exceeded for group {i + 1}, using smart analysis")
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({
                        "files": op["files"],
                        "message": op["message"],
                        "description": op.get("description", ""),
                    })
            else:
                sample = staged_files.get(normalized[0], {})
                if sample.get("deleted"):
                    msg = f"chore: remove {len(normalized)} files"
                elif sample.get("old") != normalized[0]:
                    msg = f"refactor: rename {len(normalized)} files"
                else:
                    msg = f"chore: update {len(normalized)} files"
                commits.append({"files": normalized, "message": msg, "description": ""})
            continue

        if isinstance(result, list) and len(result) > 0:
            result = result[0]
        if not isinstance(result, dict):
            warn(f"phase 2 returned non-dict for group {i + 1}, using smart analysis")
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({"files": op["files"], "message": op["message"], "description": op.get("description", "")})
            continue

        message = result.get("message")
        if not message or message.lower() in ("message", ""):
            warn(f"phase 2 missing message for group {i + 1}, using smart analysis")
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({"files": op["files"], "message": op["message"], "description": op.get("description", "")})
                continue
            message = f"chore: update {len(normalized)} files"

        description = result.get("description", "")
        if description.lower() in ("description", "desc"):
            description = ""

        commits.append({"files": normalized, "message": message, "description": description})

    return commits


def get_lockfiles_in_staging():
    """Get list of lockfiles that are staged."""
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    return [git_unquote(f) for f in staged.strip().splitlines() if f and is_lockfile(git_unquote(f))]


# ---------------------------------------------------------------------------
# Display helpers
# ---------------------------------------------------------------------------

def _fmt_files(files, limit=4):
    """Format a file list compactly for display."""
    shown = sorted(files)[:limit]
    result = ", ".join(shown)
    if len(files) > limit:
        result += f" +{len(files) - limit} more"
    return result


def _print_commit_plan(commits, lockfiles):
    """Print a numbered plan of all commits about to be made."""
    all_entries = list(commits)
    if lockfiles:
        all_entries.append({
            "message": "chore: update lockfiles",
            "description": "",
            "files": lockfiles,
        })

    sprint("")
    sprint("  Planned commits:")
    sprint("  " + "─" * 58)
    for i, c in enumerate(all_entries, 1):
        msg = c["message"]
        desc = c.get("description", "")
        files = c.get("files", [])
        sprint(f"  {i:2d}  {msg}")
        if desc:
            sprint(f"       {desc}")
        sprint(f"       {_fmt_files(files)}")
    sprint("  " + "─" * 58)
    sprint("")


# ---------------------------------------------------------------------------
# Housekeeping
# ---------------------------------------------------------------------------

KEEP_LOGS = 50


def _prune_old_logs():
    """Delete session log files beyond the most recent KEEP_LOGS."""
    if not LOGS_DIR.exists():
        return
    logs = sorted(LOGS_DIR.glob("session_*.log"))
    for old in logs[:-KEEP_LOGS]:
        try:
            old.unlink()
        except OSError:
            pass


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    global _spinner

    parser = argparse.ArgumentParser(description="Fast commit with LLM-generated messages")
    parser.add_argument("--no-verify", action="store_true", help="bypass pre-commit hooks")
    parser.add_argument("--dry-run", action="store_true", help="preview commits without executing")
    parser.add_argument("--no-push", action="store_true", help="skip automatic push")
    parser.add_argument("--bulk", action="store_true", help="skip LLM, use smart file tree analysis (for large batches)")
    args = parser.parse_args()

    log("INFO", "session started", {"dry_run": args.dry_run, "no_verify": args.no_verify, "no_push": args.no_push, "bulk": args.bulk})
    log_git_state()
    _prune_old_logs()

    config = load_config()
    log("INFO", f"using model: {config['MODEL']}")

    _spinner = Spinner()

    # --- Stage & collect ---
    _spinner.set("Staging changes...")
    diff, source = get_diff()
    if not diff:
        _spinner.stop()
        sprint("nothing to commit")
        return

    staged_files = get_staged_files_for_commit()
    file_count = len(staged_files)

    if file_count == 0:
        _spinner.stop()
        sprint("nothing to commit")
        return

    lockfiles = get_lockfiles_in_staging()
    _spinner.ok(f"Staged {file_count} file{'s' if file_count != 1 else ''}")

    audit("start", {
        "source": source,
        "file_count": file_count,
        "files": sorted(staged_files.keys()),
        "lockfiles": lockfiles,
    })

    # --- Generate commit plan ---
    if args.bulk or file_count > BULK_FILE_THRESHOLD:
        mode = "--bulk flag" if args.bulk else f"{file_count} files"
        _spinner.set(f"Grouping files ({mode})...")
        commits = group_files_by_directory(staged_files)
        _spinner.ok(f"Planned {len(commits)} commit{'s' if len(commits) != 1 else ''} (bulk)")
    else:
        missing_hint = None
        commits = None

        for attempt in range(1, MAX_LLM_RETRIES + 1):
            if file_count > LARGE_DIFF_FILES:
                _spinner.set("Grouping files...")
                raw_commits = call_llm_two_phase(config, staged_files, missing_hint=missing_hint)
            else:
                label = f"Analysing {file_count} files"
                if attempt > 1:
                    label += f" (retry {attempt}/{MAX_LLM_RETRIES})"
                _spinner.set(f"{label}...")
                raw_commits = call_llm(config, diff, missing_hint=missing_hint)

            commits, missing = validate_and_fix_commits(raw_commits, staged_files)

            if not missing:
                break

            log("INFO", f"LLM missed files on attempt {attempt}", {"missed": sorted(missing)})

            if attempt < MAX_LLM_RETRIES:
                missing_hint = missing
                warn(f"LLM missed {len(missing)} file(s), retrying ({attempt}/{MAX_LLM_RETRIES})...")
            else:
                warn(f"LLM missed files after {MAX_LLM_RETRIES} attempts, falling back to bulk mode")
                log("INFO", "falling back to bulk mode after LLM retry exhaustion", {"missed": sorted(missing)})
                commits = group_files_by_directory(staged_files)
                break

        n = len(commits)
        _spinner.ok(f"Planned {n} commit{'s' if n != 1 else ''}")

    if not commits:
        _spinner.stop()
        exit_with_error("no valid commits generated", {"file_count": file_count})

    _print_commit_plan(commits, lockfiles)

    if args.dry_run:
        _spinner.stop()
        audit("dry_run", {"commits": len(commits)})
        return

    # --- Execute commits ---
    run(["git", "reset", "HEAD"], check=True)

    commit_args = ["git", "commit"]
    if args.no_verify:
        commit_args.append("--no-verify")

    failed_commits = []
    total = len(commits)

    for idx, commit in enumerate(commits, 1):
        files = commit["files"]
        message = commit["message"]
        description = commit.get("description", "")

        short_msg = message if len(message) <= 45 else message[:42] + "..."
        _spinner.set(f"Committing {idx}/{total}: {short_msg}")

        added_files = []
        for f in files:
            if is_file_deleted(f, staged_files):
                _, stderr, code = run(["git", "rm", "--cached", "--", f])
            else:
                _, stderr, code = run(["git", "add", "--", f])
                info = staged_files.get(f, {})
                old_path = info.get("old", f)
                if code == 0 and old_path != f:
                    run(["git", "rm", "--cached", "--", old_path])
            if code == 0:
                added_files.append(f)
            else:
                warn(f"could not stage {f}: {stderr.strip()}")

        skipped_files = [f for f in files if f not in added_files]
        if skipped_files:
            audit("stage_skip", {"message": message, "skipped": skipped_files})

        if not added_files:
            _spinner.warn(f"skipping (no files staged): {message}")
            audit("commit_skip", {"message": message, "files": files})
            continue

        staged_check, _, _ = run(["git", "diff", "--cached", "--name-only"])
        if not staged_check.strip():
            warn(f"nothing staged after git add, skipping: {message}")
            audit("commit_skip", {"message": message, "files": added_files, "reason": "nothing_staged"})
            continue

        commit_cmd = commit_args + ["-m", message]
        if description:
            commit_cmd += ["-m", description]
        stdout, stderr, code = run(commit_cmd)

        if code != 0:
            error_detail = stderr.strip() or stdout.strip()
            audit("commit_fail", {"message": message, "files": added_files, "stderr": stderr.strip(), "stdout": stdout.strip()})
            if "hook" in stderr.lower() or "pre-commit" in stderr.lower():
                exit_with_error(
                    f"commit blocked by git hook. Fix the issues or use --no-verify to bypass",
                    {"message": message, "files": added_files, "stderr": stderr}
                )
            record_error(f"commit failed: {error_detail}", {"message": message, "files": added_files})
            _spinner.fail(f"FAILED: {message}")
            failed_commits.append(message)
            continue

        audit("commit_ok", {"message": message, "files": added_files})
        log("INFO", f"committed: {message}", {"files": len(added_files)})
        _spinner.ok(message)

    # Commit lockfiles separately
    if lockfiles:
        _spinner.set("Committing lockfiles...")
        run(["git", "add", "--"] + lockfiles, check=True)
        staged_check, _, _ = run(["git", "diff", "--cached", "--name-only"])
        if staged_check.strip():
            _, stderr, code = run(commit_args + ["-m", "chore: update lockfiles"])
            if code != 0:
                if "hook" in stderr.lower():
                    exit_with_error(
                        "lockfile commit blocked by git hook. Use --no-verify to bypass",
                        {"lockfiles": lockfiles, "stderr": stderr}
                    )
                exit_with_error(
                    f"lockfile commit failed: {stderr}",
                    {"lockfiles": lockfiles, "stderr": stderr}
                )
            log("INFO", "committed lockfiles", {"count": len(lockfiles)})
            _spinner.ok("chore: update lockfiles")

    if failed_commits:
        warn(f"{len(failed_commits)} commit(s) failed: {', '.join(failed_commits)}")

    # Audit remaining changes
    remaining_staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    remaining_unstaged, _, _ = run(["git", "diff", "--name-only"])
    remaining_untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
    remaining = {
        "staged": [git_unquote(f) for f in remaining_staged.strip().splitlines() if f],
        "unstaged": [git_unquote(f) for f in remaining_unstaged.strip().splitlines() if f],
        "untracked": [git_unquote(f) for f in remaining_untracked.strip().splitlines() if f],
    }
    has_remaining = any(remaining.values())
    audit("finish", {"remaining": remaining, "clean": not has_remaining, "failed_commits": failed_commits})

    run_scope = set(staged_files.keys()) | set(lockfiles)
    all_remaining = set(remaining["staged"]) | set(remaining["unstaged"]) | set(remaining["untracked"])
    missed_from_run = all_remaining & run_scope
    if missed_from_run:
        warn(f"{len(missed_from_run)} files from this run still uncommitted (check ~/.fastc/audit.jsonl)")

    if args.no_push:
        _spinner.stop()
        sprint("done (push skipped)")
        return

    _spinner.set("Pushing...")
    _, stderr, code = run(["git", "push"], timeout=120)
    if code != 0:
        if "rejected" in stderr.lower() or "non-fast-forward" in stderr.lower():
            exit_with_error(
                "push rejected: remote has changes. Run 'git pull' first to sync",
                {"stderr": stderr}
            )
        if "no upstream" in stderr.lower() or "no configured push destination" in stderr.lower():
            exit_with_error(
                "push failed: no upstream branch. Run 'git push -u origin <branch>' first",
                {"stderr": stderr}
            )
        exit_with_error("push failed", {"stderr": stderr})

    log("INFO", "pushed to remote")
    log("INFO", "session completed successfully")
    _spinner.ok("Pushed")
    _spinner.stop()


if __name__ == "__main__":
    main()
