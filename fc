#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
# ]
# ///

import argparse
import json
import subprocess
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path

CONFIG_PATH = Path.home() / ".config" / "fast-commit" / ".env"
LOGS_DIR = Path.home() / ".fastc" / "logs"
AUDIT_LOG = Path.home() / ".fastc" / "audit.jsonl"

# Session log file - created once per run
_session_log = None


def get_session_log():
    """Get or create session log file for this execution."""
    global _session_log
    if _session_log is None:
        LOGS_DIR.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        _session_log = LOGS_DIR / f"session_{timestamp}.log"
        # Write header
        _session_log.write_text(f"=== fastc session started: {datetime.now().isoformat()} ===\n")
    return _session_log


def log(level, message, context=None):
    """Log a message to the session log file.

    Levels: DEBUG, INFO, ERROR
    """
    log_file = get_session_log()
    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] {level}: {message}"
    if context:
        context_str = ", ".join(f"{k}={v}" for k, v in context.items())
        line += f" | {context_str}"
    with open(log_file, "a") as f:
        f.write(line + "\n")


MAX_STATUS_LINES_IN_LOG = 100


def log_git_state():
    """Log current git state to session log."""
    try:
        branch_result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True, timeout=5
        )
        branch = branch_result.stdout.decode().strip() if branch_result.returncode == 0 else "unknown"
        log("INFO", f"git branch: {branch}")

        status_result = subprocess.run(
            ["git", "status", "--short"],
            capture_output=True, timeout=5
        )
        if status_result.returncode == 0:
            status = status_result.stdout.decode().strip()
            if status:
                lines = status.splitlines()
                if len(lines) > MAX_STATUS_LINES_IN_LOG:
                    truncated = "\n".join(lines[:MAX_STATUS_LINES_IN_LOG])
                    log("INFO", f"git status (truncated, {len(lines)} files total):\n{truncated}\n... and {len(lines) - MAX_STATUS_LINES_IN_LOG} more")
                else:
                    log("INFO", f"git status:\n{status}")
    except Exception as e:
        log("INFO", f"failed to capture git state: {e}")


def warn(message, context=None):
    """Print warning to user and log it (uses INFO level, not WARN)."""
    print(f"  warning: {message}", file=sys.stderr)
    log("INFO", message, context)


def record_error(message, context=None):
    """Record error details to session log with full context."""
    log("ERROR", message, context)

    # Also add stack trace if we're in an exception handler
    exc_info = traceback.format_exc()
    if exc_info and exc_info.strip() != "NoneType: None":
        log_file = get_session_log()
        with open(log_file, "a") as f:
            f.write(f"Stack trace:\n{exc_info}\n")

    # Log git state for context
    log_git_state()

    print(f"  session log: {get_session_log()}", file=sys.stderr)


_audit_repo = None


def audit(event, data=None):
    """Append a structured entry to the persistent audit log.

    The audit log is a JSONL file (~/.fastc/audit.jsonl) that accumulates
    across all runs. Each line is a JSON object with timestamp, repo, event
    type, and event-specific data.
    """
    global _audit_repo
    AUDIT_LOG.parent.mkdir(parents=True, exist_ok=True)
    if _audit_repo is None:
        repo_result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True, timeout=5,
        )
        _audit_repo = repo_result.stdout.decode().strip() if repo_result.returncode == 0 else "unknown"
    entry = {
        "ts": datetime.now().isoformat(),
        "repo": _audit_repo,
        "event": event,
    }
    if data:
        entry["data"] = data
    with open(AUDIT_LOG, "a") as f:
        f.write(json.dumps(entry) + "\n")


def exit_with_error(message, context=None):
    """Print error, record to disk, and exit."""
    print(f"error: {message}", file=sys.stderr)
    record_error(message, context)
    audit("error", {"message": message})
    sys.exit(1)

# Files to exclude from diff analysis (don't help with commit messages)
EXCLUDED_PATTERNS = [
    # JS/Node lockfiles
    "pnpm-lock.yaml",
    "package-lock.json",
    "yarn.lock",
    "bun.lockb",
    # Other language lockfiles
    "Cargo.lock",
    "poetry.lock",
    "Pipfile.lock",
    "go.sum",
    "composer.lock",
    "Gemfile.lock",
    # Minified/generated files
    "*.min.js",
    "*.min.css",
    "*.map",
]

SYSTEM_PROMPT = """You are a git commit message generator. Given a git diff, analyse the changes and group them into logical atomic commits.

CRITICAL REQUIREMENTS:
- "files" MUST be actual file paths from the diff (e.g., "src/auth.go", "tests/auth_test.go")
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"
- Every file in the diff MUST appear in exactly one commit's files array

Rules:
- Each commit should represent one logical change
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single commit

BAD OUTPUT (will be rejected):
[
  {"files": "files", "message": "message", "description": "description"}
]

GOOD OUTPUT:
[
  {
    "files": ["src/auth.go", "tests/auth_test.go"],
    "message": "feat(auth): add login endpoint",
    "description": "Users need to authenticate before accessing protected resources."
  }
]

Respond with ONLY a valid JSON array (no markdown, no explanation):"""

PHASE1_PROMPT = """You are a git commit analyzer. Given a summary of changed files (name-status and stats), group them into logical atomic commits.

CRITICAL REQUIREMENT:
- "files" MUST be actual file paths from the FILE CHANGES list below

Rules:
- Each group should represent one logical change
- Group files that belong to the same logical change together
- If all changes are one logical unit, return a single group
- Use the file paths and change types (A=added, M=modified, D=deleted, R=renamed) to understand the changes

BAD OUTPUT (will be rejected):
[{"files": "files", "hint": "hint"}]

GOOD OUTPUT:
[{"files": ["src/auth.go", "tests/auth_test.go"], "hint": "authentication logic changes"}]

Respond with ONLY a valid JSON array of file groups (no markdown, no explanation):"""

PHASE2_PROMPT = """You are a git commit message generator. Given a diff for a specific group of related files, generate ONE commit message.

CRITICAL REQUIREMENTS:
- "message" MUST be a real commit message, NOT the word "message"
- "description" MUST be a real description, NOT the word "description"

Rules:
- Use conventional commit format: <type>(<scope>): <subject>
- Types: feat, fix, refactor, chore, docs, style, test, perf
- Subject: imperative mood, max 50 chars, no period
- Description: 1-3 sentences explaining WHY the change was made, not what changed

BAD OUTPUT (will be rejected):
{"message": "message", "description": "description"}

GOOD OUTPUT:
{"message": "feat(auth): add login endpoint", "description": "Users need to authenticate before accessing protected resources."}

Respond with ONLY a valid JSON object (no markdown, no explanation):"""


def load_config():
    if not CONFIG_PATH.exists():
        exit_with_error(
            f"config not found at {CONFIG_PATH}. Create it with OPENROUTER_API_KEY and MODEL",
            {"config_path": str(CONFIG_PATH)}
        )
    config = {}
    for line in CONFIG_PATH.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        key, _, value = line.partition("=")
        config[key.strip()] = value.strip().strip("\"'")
    if "OPENROUTER_API_KEY" not in config:
        exit_with_error("OPENROUTER_API_KEY not set in config", {"config_path": str(CONFIG_PATH)})
    if "MODEL" not in config:
        exit_with_error("MODEL not set in config", {"config_path": str(CONFIG_PATH)})
    return config


def run(cmd, check=False):
    result = subprocess.run(cmd, capture_output=True)
    stdout = result.stdout.decode("utf-8", errors="replace")
    stderr = result.stderr.decode("utf-8", errors="replace")
    if check and result.returncode != 0:
        exit_with_error(
            f"command failed: {' '.join(cmd)}",
            {"command": " ".join(cmd), "stderr": stderr.strip(), "returncode": result.returncode}
        )
    return stdout, stderr, result.returncode


def build_pathspec_excludes():
    """Build git pathspec exclude args for lockfiles etc."""
    excludes = []
    for pattern in EXCLUDED_PATTERNS:
        excludes.extend([":(exclude)" + pattern])
    return excludes


def strip_diff_noise(diff):
    """Remove noise lines from diff to save tokens."""
    lines = []
    for line in diff.splitlines():
        if line.startswith("index "):
            continue
        if line.startswith("similarity index "):
            continue
        if line.startswith("dissimilarity index "):
            continue
        lines.append(line)
    return "\n".join(lines)


def get_diff():
    """Get diff for analysis, excluding lockfiles from LLM context."""
    excludes = build_pathspec_excludes()
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    if staged.strip():
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        diff = strip_diff_noise(diff)
        if diff:
            return diff, "staged"
        # Staged files were all lockfiles/excluded — fall through to check unstaged
    diff, _, _ = run(["git", "diff", "--minimal"])
    if not diff.strip():
        untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
        if not untracked.strip():
            return "", "none"
        run(["git", "add", "-A"], check=True)
        diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
        return strip_diff_noise(diff), "all"
    run(["git", "add", "-A"], check=True)
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + excludes)
    return strip_diff_noise(diff), "all"


MAX_DIFF_CHARS = 20000
MAX_LINES_PER_HUNK = 15
MAX_HUNKS_PER_FILE = 5
LARGE_DIFF_FILES = 15
MAX_OUTPUT_TOKENS = 4096
# Beyond this many files, skip LLM entirely and use bulk commit
BULK_FILE_THRESHOLD = 200
# Maximum characters for phase 1 file summary before fallback
MAX_PHASE1_CHARS = 50000


def parse_files_from_diff(diff):
    """Extract the set of files mentioned in a diff."""
    files = set()
    for line in diff.splitlines():
        if line.startswith("diff --git a/"):
            parts = line.split(" b/")
            if len(parts) == 2:
                files.add(parts[1])
        elif line.startswith("rename from "):
            files.add(line[12:])
        elif line.startswith("rename to "):
            files.add(line[10:])
    return files


def is_lockfile(path):
    """Check if a file path matches any lockfile pattern."""
    for pattern in EXCLUDED_PATTERNS:
        if pattern.startswith("*"):
            if path.endswith(pattern[1:]):
                return True
        elif path.endswith("/" + pattern) or path == pattern:
            return True
    return False


def git_unquote(path):
    """Decode git's C-style quoted path.

    Git quotes paths with non-ASCII or special chars using octal escapes:
    "path/Age of Empires\\357\\274\\232 stuff" -> path/Age of Empires： stuff
    """
    if not (path.startswith('"') and path.endswith('"')):
        return path
    inner = path[1:-1]
    result = bytearray()
    i = 0
    while i < len(inner):
        if inner[i] == '\\' and i + 1 < len(inner):
            c = inner[i + 1]
            if c in '01234567' and i + 3 < len(inner):
                result.append(int(inner[i + 1:i + 4], 8))
                i += 4
            elif c == '\\':
                result.append(ord('\\'))
                i += 2
            elif c == '"':
                result.append(ord('"'))
                i += 2
            elif c == 'n':
                result.append(ord('\n'))
                i += 2
            elif c == 't':
                result.append(ord('\t'))
                i += 2
            else:
                result.extend(inner[i].encode('utf-8'))
                i += 1
        else:
            result.extend(inner[i].encode('utf-8'))
            i += 1
    return result.decode('utf-8')


def get_staged_files_for_commit():
    """Get actual staged files that can be committed (excluding lockfiles).

    Returns dict mapping path -> {old_path, is_deleted}
    This ensures we use paths that actually exist for git add/rm.
    Lockfiles are excluded since they're committed separately.
    """
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M"])
    files = {}
    for line in name_status.strip().splitlines():
        if not line:
            continue
        parts = line.split("\t")
        status = parts[0]
        if status.startswith("R"):  # Rename: R100\told\tnew
            old_path = git_unquote(parts[1])
            new_path = git_unquote(parts[2])
            if not is_lockfile(new_path):
                files[new_path] = {"old": old_path, "deleted": False}
        elif status == "D":  # Deleted
            path = git_unquote(parts[1])
            if not is_lockfile(path):
                files[path] = {"old": path, "deleted": True}
        else:  # Added, Modified, etc
            path = git_unquote(parts[1])
            if not is_lockfile(path):
                files[path] = {"old": path, "deleted": False}
    return files


def normalize_llm_files(llm_files, staged_files):
    """Map LLM file paths to actual staged paths.

    The LLM might return old paths for renamed files or paths that don't exist.
    This maps them to actual paths that git can work with.
    Returns deduplicated list.
    """
    if not isinstance(llm_files, list):
        return []

    # Build reverse lookup: old_path -> new_path
    old_to_new = {info["old"]: path for path, info in staged_files.items()}

    normalized = set()
    for f in llm_files:
        if not isinstance(f, str):
            continue
        if f in staged_files:
            normalized.add(f)
        elif f in old_to_new:
            normalized.add(old_to_new[f])
    return list(normalized)


def is_file_deleted(path, staged_files):
    """Check if a file is marked as deleted in staged files."""
    info = staged_files.get(path)
    return info["deleted"] if info else False


def is_echoing_keys(commit):
    """Check if commit has literal key strings instead of real values."""
    if not isinstance(commit, dict):
        return True
    files = commit.get("files")
    message = commit.get("message")
    description = commit.get("description")

    # Check if files is invalid
    if files is None:
        return True
    if isinstance(files, str) and files in ("files", "[]", "", "file"):
        return True
    if isinstance(files, list) and len(files) == 0:
        return True

    # Check if message is just echoing the key name
    if message is None:
        return True
    if isinstance(message, str) and message.lower() in ("message", "", "commit message", "msg"):
        return True

    # Check if description is just echoing the key name
    if isinstance(description, str) and description.lower() in ("description", "desc"):
        return True

    return False


def validate_and_fix_commits(commits, staged_files):
    """Validate LLM output and fix issues.

    - Normalizes file paths (handles renames)
    - Removes invalid files
    - Errors if any files are missing from groupings
    - Returns fixed list of commits
    """
    # Detect key-echoing pattern (model failure)
    echo_count = sum(1 for c in commits if is_echoing_keys(c))
    if echo_count > 0 and echo_count == len(commits):
        exit_with_error(
            f"LLM returned malformed commits (echoing keys: {echo_count}/{len(commits)})",
            {"commits_sample": commits[:3]}
        )

    all_staged = set(staged_files.keys())
    covered_files = set()
    fixed_commits = []

    for commit in commits:
        if not isinstance(commit, dict):
            warn(f"skipping non-dict commit: {commit}")
            continue
        if "files" not in commit or "message" not in commit:
            warn(f"skipping malformed commit: {commit}")
            continue

        normalized = normalize_llm_files(commit["files"], staged_files)
        if not normalized:
            warn(f"skipping commit with no valid files: {commit.get('message', '<missing>')}")
            continue

        covered_files.update(normalized)
        fixed_commits.append({
            "files": normalized,
            "message": commit["message"],
            "description": commit.get("description", ""),
        })

    missing_files = all_staged - covered_files
    if missing_files:
        # Add missing files to the last commit or create a new one
        if fixed_commits:
            warn(f"LLM missed {len(missing_files)} files, adding to last commit", {"files": list(missing_files)[:5]})
            fixed_commits[-1]["files"].extend(list(missing_files))
        else:
            # No valid commits at all - create a fallback single commit
            warn("LLM failed to group files, creating single commit")
            fixed_commits.append({
                "files": list(all_staged),
                "message": "chore: update files",
                "description": "",
            })

    return fixed_commits


def get_diff_for_files(files):
    """Get the diff for specific files only."""
    diff, _, _ = run(["git", "diff", "--cached", "--minimal", "--"] + list(files))
    return strip_diff_noise(diff)


def compress_diff(diff):
    """Compress diff while preserving semantic information.

    Strategy:
    - Include --stat and --dirstat for overview
    - Keep all hunk headers (@@...@@) which show function context
    - Limit lines per hunk, prioritizing additions (+) over context
    - Limit total hunks per file
    """
    excludes = build_pathspec_excludes()
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)
    dirstat, _, _ = run(["git", "diff", "--cached", "--dirstat", "--"] + excludes)

    parts = [f"DIFF STAT:\n{stat}"]
    if dirstat.strip():
        parts.append(f"DIRECTORY CHANGES:\n{dirstat}")
    parts.append("COMPRESSED PATCHES:")

    current_file_header = None
    current_file_lines = []
    hunk_count = 0
    hunk_lines = 0
    hunk_truncated = False

    def flush_file():
        nonlocal current_file_header, current_file_lines, hunk_count
        if current_file_header:
            parts.append(current_file_header)
            parts.extend(current_file_lines)
            if hunk_count > MAX_HUNKS_PER_FILE:
                parts.append(f"[... {hunk_count - MAX_HUNKS_PER_FILE} more hunks truncated]")
        current_file_header = None
        current_file_lines = []
        hunk_count = 0

    for line in diff.splitlines():
        if line.startswith("diff --git"):
            flush_file()
            current_file_header = line
            hunk_count = 0
        elif line.startswith("@@") and " @@" in line:
            hunk_count += 1
            hunk_lines = 0
            hunk_truncated = False
            if hunk_count <= MAX_HUNKS_PER_FILE:
                current_file_lines.append(line)
        elif current_file_header and hunk_count <= MAX_HUNKS_PER_FILE:
            if line.startswith("---") or line.startswith("+++"):
                current_file_lines.append(line)
            elif line.startswith("rename ") or line.startswith("new file") or line.startswith("deleted file"):
                current_file_lines.append(line)
            elif hunk_lines < MAX_LINES_PER_HUNK:
                if line.startswith("+") or line.startswith("-"):
                    current_file_lines.append(line)
                    hunk_lines += 1
                elif line.startswith(" ") and hunk_lines < MAX_LINES_PER_HUNK // 2:
                    current_file_lines.append(line)
                    hunk_lines += 1
            elif not hunk_truncated:
                current_file_lines.append("[... hunk truncated]")
                hunk_truncated = True

    flush_file()
    return "\n".join(parts)


RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}
MAX_RETRIES = 3


class ContextLengthExceeded(Exception):
    """Raised when LLM context length is exceeded."""
    pass


def try_repair_json(content):
    """Attempt to repair truncated or malformed JSON arrays/objects.

    Common issues:
    - Leading garbage: .[{...  or ```json[{...
    - Trailing garbage: }]...  or }]```
    - Truncated mid-string: [..., "file  -> close string and array
    - Truncated mid-object: [..., {"files": ["a"]  -> close object and array
    - Object instead of array: {"files": [...]} when we expected [{...}]
    """
    content = content.strip()

    # Strip leading garbage - find first [ or {
    start_idx = -1
    for i, c in enumerate(content):
        if c in '[{':
            start_idx = i
            break
    if start_idx > 0:
        content = content[start_idx:]

    # Strip trailing garbage - find last ] or }
    end_idx = -1
    for i in range(len(content) - 1, -1, -1):
        if content[i] in ']}':
            end_idx = i
            break
    if end_idx >= 0 and end_idx < len(content) - 1:
        content = content[:end_idx + 1]

    # Try as-is first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try adding closing brackets/braces
    repairs = [
        '"}]',      # truncated mid-string in array
        '"]}}]',    # truncated mid-array in object in array
        '"}]}]',    # truncated mid-string in nested structure
        '"]',       # truncated mid-array
        '"}',       # truncated mid-string in object
        '}]',       # truncated mid-object in array
        ']',        # truncated array
        '}',        # truncated object
        '"]}',      # truncated mid-string in array inside object (phase1 {"files": ["a", "b)
        '"],"hint":""}]',  # phase1 truncated with hint missing
    ]

    for repair in repairs:
        try:
            result = json.loads(content + repair)
            print(f"  repaired truncated JSON with: {repair}")
            return result
        except json.JSONDecodeError:
            continue

    # Try removing the last incomplete element
    # Find the last complete element (ends with }, or ])
    for end_char in ['},', '],', '}', ']', '",']:
        idx = content.rfind(end_char)
        if idx > 0:
            truncated = content[:idx + len(end_char)]
            # Add closing brackets as needed
            for close in [']', ']}', '}]', ']}]', '"]}', '"],"hint":""}]']:
                try:
                    result = json.loads(truncated + close)
                    print(f"  repaired truncated JSON by removing incomplete element")
                    return result
                except json.JSONDecodeError:
                    continue

    return None


def call_llm_raw(config, system_prompt, user_content):
    """Make a raw LLM API call and return parsed JSON."""
    import requests

    use_structured = config.get("STRUCTURED_OUTPUT", "true").lower() == "true"

    request_body = {
        "model": config["MODEL"],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
        "temperature": 0,
        "max_tokens": MAX_OUTPUT_TOKENS,
    }
    if use_structured:
        request_body["response_format"] = {"type": "json_object"}

    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            resp = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config['OPENROUTER_API_KEY']}",
                    "Content-Type": "application/json",
                },
                json=request_body,
                timeout=60,
            )
            if resp.status_code == 200:
                break
            if resp.status_code == 400 and "context length" in resp.text.lower():
                # Context length exceeded - signal caller to use fallback
                log("INFO", "LLM context length exceeded", {"response": resp.text[:300]})
                raise ContextLengthExceeded(resp.text[:500])
            if resp.status_code in RETRYABLE_STATUS_CODES:
                last_error = f"API returned {resp.status_code}: {resp.text[:200]}"
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2**attempt
                    warn(f"LLM API returned {resp.status_code}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                    time.sleep(wait_time)
                    continue
            exit_with_error(
                f"LLM API returned {resp.status_code}",
                {"status_code": resp.status_code, "response": resp.text[:500], "model": config["MODEL"]}
            )
        except requests.exceptions.Timeout:
            last_error = "request timed out"
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request timed out, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request timed out after {MAX_RETRIES} attempts",
                {"model": config["MODEL"], "attempts": MAX_RETRIES}
            )
        except requests.exceptions.RequestException as e:
            last_error = str(e)
            if attempt < MAX_RETRIES - 1:
                wait_time = 2**attempt
                warn(f"LLM request failed: {e}, retrying in {wait_time}s (attempt {attempt + 1}/{MAX_RETRIES})")
                time.sleep(wait_time)
                continue
            exit_with_error(
                f"LLM API request failed after {MAX_RETRIES} attempts: {e}",
                {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": str(e)}
            )
    else:
        exit_with_error(
            f"LLM API failed after {MAX_RETRIES} attempts: {last_error}",
            {"model": config["MODEL"], "attempts": MAX_RETRIES, "last_error": last_error}
        )
    try:
        resp_json = resp.json()
    except json.JSONDecodeError as e:
        exit_with_error(
            f"failed to parse API response as JSON: {e}",
            {"response_text": resp.text[:1000], "model": config["MODEL"]}
        )
    try:
        content = resp_json["choices"][0]["message"]["content"]
    except (KeyError, IndexError) as e:
        exit_with_error(
            f"unexpected API response structure: {e}",
            {"response_json": str(resp_json)[:1000], "model": config["MODEL"]}
        )
    content = content.strip()
    if content.startswith("```"):
        content = "\n".join(content.split("\n")[1:])
    if content.endswith("```"):
        content = "\n".join(content.split("\n")[:-1])
    content = content.strip()

    # Check if response was truncated (finish_reason)
    finish_reason = resp_json.get("choices", [{}])[0].get("finish_reason", "")
    if finish_reason == "length":
        warn("LLM response was truncated due to length limit")

    # Handle empty response - return empty list to trigger fallback/retry logic
    if not content:
        record_error("LLM returned empty response", {"model": config["MODEL"]})
        return []

    # Try normal parse first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try to repair truncated JSON
    repaired = try_repair_json(content)
    if repaired is not None:
        return repaired

    # Return empty list instead of exiting - allows caller to fall back gracefully
    record_error(
        "LLM returned unparseable JSON",
        {"content": content[:1000], "model": config["MODEL"]}
    )
    return []


RETRY_PROMPT = """You MUST output REAL file paths and REAL commit messages.

Example of what NOT to do (will be rejected):
[{"files": "files", "message": "message", "description": "description"}]

Example of what you MUST do:
[{"files": ["src/main.go", "src/config.go"], "message": "feat(config): add app settings", "description": "Add runtime configuration support"}]

CRITICAL: Every file in the diff below must appear in the files array.
Output ONLY valid JSON array with no markdown fences:"""


def is_malformed_response(result):
    """Check if response is clearly malformed (key echoing or invalid structure)."""
    if not isinstance(result, list):
        return True
    if not result:
        return True
    # Check if all commits have key-echoing pattern
    echo_count = sum(1 for r in result if is_echoing_keys(r))
    return echo_count > len(result) // 2


def normalize_llm_result(result):
    """Normalize LLM result to a list of commits.

    Handles various response formats:
    - Direct array: [{...}, {...}]
    - Object with commits key: {"commits": [{...}]}
    - Object with groups key: {"groups": [{...}]}
    - Single commit object: {"files": [...], "message": "..."}
    """
    if isinstance(result, list):
        return result
    if isinstance(result, dict):
        if "commits" in result:
            return result["commits"]
        if "groups" in result:
            return result["groups"]
        if "files" in result and "message" in result:
            return [result]
    return []


def call_llm(config, diff):
    """Single-phase: send diff and get commits."""
    content = diff if len(diff) <= MAX_DIFF_CHARS else compress_diff(diff)
    log("INFO", f"sending diff to LLM", {"chars": len(content), "compressed": len(diff) > MAX_DIFF_CHARS})
    result = call_llm_raw(config, SYSTEM_PROMPT, content)
    result = normalize_llm_result(result)

    # Retry with stronger prompt if response looks malformed
    if is_malformed_response(result):
        warn("response looks malformed, retrying with stronger prompt")
        # Use compressed version for retry to save tokens
        retry_content = compress_diff(diff) if len(diff) > MAX_DIFF_CHARS // 2 else content
        result = call_llm_raw(config, RETRY_PROMPT, retry_content)
        result = normalize_llm_result(result)

    return result


def detect_directory_moves(staged_files):
    """Detect directory-level rename/move operations.

    Analyzes rename patterns to find cases like:
    - tla-dump/a/x.txt -> refs/tla-dump/a/x.txt (move tla-dump/ under refs/)
    - old-name/x.txt -> new-name/x.txt (rename directory)

    Returns dict of {destination_prefix: (source_prefix, [files])}
    """
    from collections import defaultdict

    # Group renames by the transformation pattern
    # Key: (source_prefix, dest_prefix), Value: list of file paths
    transforms = defaultdict(list)

    for path, info in staged_files.items():
        old_path = info.get("old", path)
        if old_path == path or info.get("deleted"):
            continue  # Not a rename

        old_parts = old_path.split("/")
        new_parts = path.split("/")

        # Find longest common suffix (the unchanged part of the path)
        common_suffix_len = 0
        for i in range(1, min(len(old_parts), len(new_parts)) + 1):
            if old_parts[-i] == new_parts[-i]:
                common_suffix_len = i
            else:
                break

        if common_suffix_len > 0:
            old_prefix = "/".join(old_parts[:-common_suffix_len])
            new_prefix = "/".join(new_parts[:-common_suffix_len])
        else:
            # No common suffix, use parent directories
            old_prefix = "/".join(old_parts[:-1])
            new_prefix = "/".join(new_parts[:-1])

        transforms[(old_prefix, new_prefix)].append(path)

    # Filter to significant moves (3+ files)
    return {
        new_prefix: (old_prefix, files)
        for (old_prefix, new_prefix), files in transforms.items()
        if len(files) >= 3
    }


def summarize_file_operations(staged_files):
    """Analyze staged files and return high-level operation summaries.

    Returns list of dicts with:
    - files: list of file paths
    - message: commit message
    - description: optional description
    """
    from collections import defaultdict

    operations = []
    handled_files = set()

    # 1. Detect directory moves/renames
    dir_moves = detect_directory_moves(staged_files)
    for new_prefix, (old_prefix, files) in sorted(dir_moves.items(), key=lambda x: -len(x[1])):
        # Skip already handled files
        files = [f for f in files if f not in handled_files]
        if len(files) < 3:
            continue

        handled_files.update(files)

        # Generate message
        old_name = old_prefix.split("/")[-1] if old_prefix else "files"
        new_name = new_prefix.split("/")[-1] if new_prefix else "root"

        if old_prefix and new_prefix:
            if old_name == new_name:
                # Same name, different location = move
                msg = f"refactor: move {old_prefix}/ to {new_prefix}/"
            else:
                # Different name = rename
                msg = f"refactor: rename {old_prefix}/ to {new_prefix}/"
        elif new_prefix:
            msg = f"refactor: move files into {new_prefix}/"
        else:
            msg = f"refactor: move {old_prefix}/ to root"

        operations.append({
            "files": files,
            "message": msg,
            "description": f"{len(files)} files moved"
        })

    # 2. Group remaining files by operation type and directory
    remaining = {p: info for p, info in staged_files.items() if p not in handled_files}

    # Categorize by operation
    by_op_and_dir = defaultdict(lambda: defaultdict(list))
    for path, info in remaining.items():
        if info.get("deleted"):
            op = "delete"
        elif info.get("old") != path:
            op = "rename"
        else:
            op = "add"

        # Get directory (try different levels of granularity)
        parts = path.split("/")
        if len(parts) > 2:
            dir_key = "/".join(parts[:2])  # Two levels deep
        elif len(parts) > 1:
            dir_key = parts[0]
        else:
            dir_key = "root"

        by_op_and_dir[op][dir_key].append(path)

    # Generate operations for deletions
    for dir_key, files in by_op_and_dir["delete"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"chore: remove {dir_key}/",
                "description": f"{len(files)} files deleted"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"chore: remove {fname}",
                    "description": ""
                })
                handled_files.add(f)

    # Generate operations for additions
    for dir_key, files in by_op_and_dir["add"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"feat: add {dir_key}/",
                "description": f"{len(files)} files added"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"feat: add {fname}",
                    "description": ""
                })
                handled_files.add(f)

    # Generate operations for remaining renames (not part of a dir move)
    for dir_key, files in by_op_and_dir["rename"].items():
        if len(files) >= 3:
            operations.append({
                "files": files,
                "message": f"refactor: rename files in {dir_key}/",
                "description": f"{len(files)} files renamed"
            })
            handled_files.update(files)
        else:
            for f in files:
                fname = f.split("/")[-1]
                operations.append({
                    "files": [f],
                    "message": f"refactor: rename {fname}",
                    "description": ""
                })
                handled_files.add(f)

    return operations


def group_files_by_directory(staged_files):
    """Smart grouping that detects directory-level operations.

    Uses file tree analysis to understand bulk operations like:
    - Directory renames/moves
    - Bulk deletions
    - Bulk additions

    Returns list of groups with files and meaningful commit messages.
    """
    operations = summarize_file_operations(staged_files)

    if operations:
        return operations

    # Ultimate fallback: single commit with all files
    return [{
        "files": list(staged_files.keys()),
        "message": "chore: update files",
        "description": f"{len(staged_files)} files changed"
    }]


def call_llm_two_phase(config, staged_files):
    """Two-phase approach for large diffs with many files.

    Phase 1: Send file summary (name-status + stat) to get logical groupings
    Phase 2: For each group, send actual diff to generate commit message
    """
    excludes = build_pathspec_excludes()
    name_status, _, _ = run(["git", "diff", "--cached", "--name-status", "-M", "--"] + excludes)
    stat, _, _ = run(["git", "diff", "--cached", "--stat", "--"] + excludes)

    phase1_content = f"FILE CHANGES:\n{name_status}\nSTATISTICS:\n{stat}"

    # Check if phase1 content is too large for the LLM
    if len(phase1_content) > MAX_PHASE1_CHARS:
        warn(f"file summary too large ({len(phase1_content)} chars), grouping by directory instead")
        log("INFO", "skipping LLM phase 1 due to size", {"chars": len(phase1_content)})
        groups = group_files_by_directory(staged_files)
    else:
        log("INFO", "phase 1: sending file summary to LLM", {"chars": len(phase1_content)})
        print("  phase 1: grouping files...")
        try:
            groups = call_llm_raw(config, PHASE1_PROMPT, phase1_content)
        except ContextLengthExceeded:
            warn("LLM context length exceeded, falling back to directory grouping")
            groups = group_files_by_directory(staged_files)

    # Handle case where LLM returns a dict with "groups" key instead of array
    if isinstance(groups, dict):
        groups = groups.get("groups", groups.get("commits", []))

    if not isinstance(groups, list):
        warn(f"phase1 returned non-list ({type(groups).__name__}), using smart grouping")
        groups = group_files_by_directory(staged_files)

    if not groups:
        # Use smart grouping as fallback
        groups = group_files_by_directory(staged_files)

    commits = []
    for i, group in enumerate(groups):
        if not isinstance(group, dict):
            continue
        files = group.get("files", [])
        if not files:
            continue

        # If group already has a message (from smart grouping), use it directly
        if group.get("message"):
            normalized = normalize_llm_files(files, staged_files) if isinstance(files[0], str) else files
            if normalized:
                commits.append({
                    "files": normalized,
                    "message": group["message"],
                    "description": group.get("description", ""),
                })
            continue

        # Normalize files to actual staged paths
        normalized = normalize_llm_files(files, staged_files)
        if not normalized:
            continue

        print(f"  phase 2: generating message for group {i + 1}/{len(groups)}...")
        group_diff = get_diff_for_files(normalized)
        if not group_diff.strip():
            continue
        content = group_diff if len(group_diff) <= MAX_DIFF_CHARS else compress_diff(group_diff)
        log("INFO", f"phase 2 group {i + 1}: sending diff to LLM", {"chars": len(content), "files": len(normalized)})

        try:
            result = call_llm_raw(config, PHASE2_PROMPT, content)
        except ContextLengthExceeded:
            warn(f"context length exceeded for group {i + 1}, using smart analysis")
            # Build staged_files subset for this group and analyze
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({
                        "files": op["files"],
                        "message": op["message"],
                        "description": op.get("description", ""),
                    })
            else:
                # Ultimate fallback: describe based on operation type
                sample = staged_files.get(normalized[0], {})
                if sample.get("deleted"):
                    msg = f"chore: remove {len(normalized)} files"
                elif sample.get("old") != normalized[0]:
                    msg = f"refactor: rename {len(normalized)} files"
                else:
                    msg = f"chore: update {len(normalized)} files"
                commits.append({"files": normalized, "message": msg, "description": ""})
            continue

        # Handle various response formats - extract dict from list if needed
        if isinstance(result, list) and len(result) > 0:
            result = result[0]
        if not isinstance(result, dict):
            warn(f"phase2 returned non-dict for group {i + 1}, using smart analysis")
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({"files": op["files"], "message": op["message"], "description": op.get("description", "")})
            continue

        message = result.get("message")
        if not message or message.lower() in ("message", ""):
            warn(f"phase2 missing message for group {i + 1}, using smart analysis")
            group_staged = {f: staged_files[f] for f in normalized if f in staged_files}
            smart_ops = summarize_file_operations(group_staged)
            if smart_ops:
                for op in smart_ops:
                    commits.append({"files": op["files"], "message": op["message"], "description": op.get("description", "")})
                continue
            # If smart analysis also fails, describe based on file count
            message = f"chore: update {len(normalized)} files"

        description = result.get("description", "")
        if description.lower() in ("description", "desc"):
            description = ""

        commits.append({"files": normalized, "message": message, "description": description})

    return commits


def get_lockfiles_in_staging():
    """Get list of lockfiles that are staged."""
    staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    return [git_unquote(f) for f in staged.strip().splitlines() if f and is_lockfile(git_unquote(f))]


def main():
    parser = argparse.ArgumentParser(description="Fast commit with LLM-generated messages")
    parser.add_argument("--no-verify", action="store_true", help="bypass pre-commit hooks")
    parser.add_argument("--dry-run", action="store_true", help="preview commits without executing")
    parser.add_argument("--no-push", action="store_true", help="skip automatic push")
    parser.add_argument("--bulk", action="store_true", help="skip LLM, use smart file tree analysis (for large batches)")
    args = parser.parse_args()

    log("INFO", "session started", {"dry_run": args.dry_run, "no_verify": args.no_verify, "no_push": args.no_push, "bulk": args.bulk})
    log_git_state()

    config = load_config()
    log("INFO", f"using model: {config['MODEL']}")

    diff, source = get_diff()
    if not diff:
        print("nothing to commit")
        return

    staged_files = get_staged_files_for_commit()
    file_count = len(staged_files)

    if file_count == 0:
        print("nothing to commit")
        return

    # Check for lockfiles before we reset staging (must happen before reset)
    lockfiles = get_lockfiles_in_staging() if source == "all" else []

    audit("start", {
        "source": source,
        "file_count": file_count,
        "files": sorted(staged_files.keys()),
        "lockfiles": lockfiles,
    })

    print(f"analysing {source} changes ({file_count} files)...")

    # For very large file counts or explicit --bulk flag, use smart file tree analysis
    if args.bulk or file_count > BULK_FILE_THRESHOLD:
        if args.bulk:
            print("  using smart bulk mode (--bulk flag)")
            log("INFO", "using bulk mode (--bulk flag)", {"file_count": file_count})
        else:
            print(f"  large changeset ({file_count} files), using smart bulk mode")
            log("INFO", "using bulk mode (auto)", {"file_count": file_count})

        # Use smart file tree analysis to detect directory-level operations
        commits = group_files_by_directory(staged_files)
        print(f"  detected {len(commits)} logical operations")
    elif file_count > LARGE_DIFF_FILES:
        print("  using two-phase approach for large diff")
        commits = call_llm_two_phase(config, staged_files)
        commits = validate_and_fix_commits(commits, staged_files)
    else:
        commits = call_llm(config, diff)
        commits = validate_and_fix_commits(commits, staged_files)

    if not commits:
        exit_with_error("no valid commits generated", {"file_count": file_count})

    if args.dry_run:
        print("\ndry run - would create these commits:")
        for commit in commits:
            print(f"  {commit['message']}")
            if commit.get("description"):
                print(f"    {commit['description']}")
            for f in commit["files"]:
                print(f"    - {f}")
        if lockfiles:
            print(f"  chore: update lockfiles")
            for f in lockfiles:
                print(f"    - {f}")
        audit("dry_run", {"commits": len(commits)})
        return

    if source == "all":
        run(["git", "reset", "HEAD"], check=True)

    commit_args = ["git", "commit"]
    if args.no_verify:
        commit_args.append("--no-verify")

    for commit in commits:
        files = commit["files"]
        message = commit["message"]
        description = commit.get("description", "")

        # Stage files: git rm for deletes, git add for adds/modifies,
        # and for renames also rm the old path from the index
        added_files = []
        for f in files:
            if is_file_deleted(f, staged_files):
                _, stderr, code = run(["git", "rm", "--cached", "--", f])
            else:
                _, stderr, code = run(["git", "add", "--", f])
                # For renames: also remove the old path from the index
                info = staged_files.get(f, {})
                old_path = info.get("old", f)
                if code == 0 and old_path != f:
                    run(["git", "rm", "--cached", "--", old_path])
            if code == 0:
                added_files.append(f)
            else:
                warn(f"could not stage {f}: {stderr.strip()}")

        skipped_files = [f for f in files if f not in added_files]
        if skipped_files:
            audit("stage_skip", {"message": message, "skipped": skipped_files})

        if not added_files:
            print(f"  skipping commit (no files staged): {message}")
            audit("commit_skip", {"message": message, "files": files})
            continue

        # Use two -m flags: first for subject, second for body
        commit_cmd = commit_args + ["-m", message]
        if description:
            commit_cmd += ["-m", description]
        _, stderr, code = run(commit_cmd)
        if code != 0:
            audit("commit_fail", {"message": message, "files": added_files, "stderr": stderr.strip()})
            # Detect hook failures
            if "hook" in stderr.lower() or "pre-commit" in stderr.lower():
                exit_with_error(
                    f"commit blocked by git hook. Fix the issues or use --no-verify to bypass",
                    {"message": message, "files": added_files, "stderr": stderr}
                )
            exit_with_error(
                f"commit failed: {stderr}",
                {"message": message, "files": added_files, "stderr": stderr}
            )
        audit("commit_ok", {"message": message, "files": added_files})
        log("INFO", f"committed: {message}", {"files": len(added_files)})
        print(f"  {message}")

    # Commit lockfiles separately with a simple message
    if lockfiles:
        run(["git", "add", "--"] + lockfiles, check=True)
        # Verify something is actually staged before committing
        staged_check, _, _ = run(["git", "diff", "--cached", "--name-only"])
        if staged_check.strip():
            _, stderr, code = run(commit_args + ["-m", "chore: update lockfiles"])
            if code != 0:
                if "hook" in stderr.lower():
                    exit_with_error(
                        f"lockfile commit blocked by git hook. Use --no-verify to bypass",
                        {"lockfiles": lockfiles, "stderr": stderr}
                    )
                exit_with_error(
                    f"lockfile commit failed: {stderr}",
                    {"lockfiles": lockfiles, "stderr": stderr}
                )
            log("INFO", "committed lockfiles", {"count": len(lockfiles)})
            print("  chore: update lockfiles")

    # Audit: check for remaining uncommitted changes
    remaining_staged, _, _ = run(["git", "diff", "--cached", "--name-only"])
    remaining_unstaged, _, _ = run(["git", "diff", "--name-only"])
    remaining_untracked, _, _ = run(["git", "ls-files", "--others", "--exclude-standard"])
    remaining = {
        "staged": [git_unquote(f) for f in remaining_staged.strip().splitlines() if f],
        "unstaged": [git_unquote(f) for f in remaining_unstaged.strip().splitlines() if f],
        "untracked": [git_unquote(f) for f in remaining_untracked.strip().splitlines() if f],
    }
    has_remaining = any(remaining.values())
    audit("finish", {"remaining": remaining, "clean": not has_remaining})
    if has_remaining:
        total = sum(len(v) for v in remaining.values())
        warn(f"{total} files still uncommitted after run (check ~/.fastc/audit.jsonl)")

    if args.no_push:
        log("INFO", "session completed (push skipped)")
        print("done (push skipped)")
        return

    _, stderr, code = run(["git", "push"])
    if code != 0:
        # Detect out-of-sync (rejected push)
        if "rejected" in stderr.lower() or "non-fast-forward" in stderr.lower():
            exit_with_error(
                "push rejected: remote has changes. Run 'git pull' first to sync",
                {"stderr": stderr}
            )
        if "no upstream" in stderr.lower() or "no configured push destination" in stderr.lower():
            exit_with_error(
                "push failed: no upstream branch. Run 'git push -u origin <branch>' first",
                {"stderr": stderr}
            )
        exit_with_error("push failed", {"stderr": stderr})
    log("INFO", "pushed to remote")
    log("INFO", "session completed successfully")
    print("pushed")


if __name__ == "__main__":
    main()

